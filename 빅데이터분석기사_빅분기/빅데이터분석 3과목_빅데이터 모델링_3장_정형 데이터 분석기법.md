
## 분류분석
### 로지스틱 회귀 분석(Logistic Regression Analysis)
*범주형인 경우에 적용할 수 있는 회귀분석 모형임을 알아두기. 주요특징/시그모이드함수*
가. 개념
- 반응변수(종속변수)가 범주형인 경우에 적용할 수 있는 회귀분석 모형
- 데이터의 반응변수가 특정 범주(또는 집단)에 속할 확률을 0에서 1사이의 값으로 예측하고, 예측된 확률에 따라 가능성이 더 높은 범주로 분류하는 지도학습 알고리즘

나. 로지스틱 회귀분석의 원리
- 반응변수 Y가 범주형 변수인 경우, 일반적인 선형회귀모형으로는 값을 바로 추정할 수 없음
- Y가 연속형 변수이고 정규성을 만족할 때 선형 회귀모형은 E(Y) = b0+b1x1 + ... + bkxk와 값이 나타낼 수 있는데, Y가 0과 1로 이루어진 이진형 반응변수는 이가 성립하지 않는다
- 이는 좌변은 E(Y)=P(Y=1)로 0과 1 사이의 확률 값(이진형 반응변수일 때) 이고, 우변은 (-∞, ∞)사이의 값을 가지기 때문이다
- 따라서 로지스틱 회귀분석에서는 범주형 변수 Y의 기댓값 E(Y)=P(Y=1)가 (-∞, ∞)사이의 값을 가질 수 있도록 로짓변환을 실시한다
- 로짓변환 : 종속 변수의 값을 확률로 변환하는 과정
- log P(Y=1)/1-P(Y=1

1) 오즈(odds)
- Y=1인 경우가 성공일 때, 오즈는 ![[Pasted image 20240320173628.png]]
- 오즈는 0에서 무한대 사이의 값을 가짐

2) 로그오즈(Log-Odds)
- 성공 확률과 실패 확률의 비율에 대한 로그를 취한 값
- 오즈에 로그함수를 적용시킨 것으로 (-∞, ∞)사이의 값을 가짐

3) **시그모이드 함수** (확률의 계산)
- 하지만 로지스틱 회귀분석을 통해 알고 싶은 것은 로그 오즈 값이 아니라 반응변수 값이 성공(1)인지 실패(0)인지를 분류하기 위한 성공확률 값임
- 따라서 확률 P값을 추정할 수 있도록 식 1을 P에 대하여 정리해야 하며, 그 형태는 아래 식과 같다. 이를 시그모이드 함수라고 하며, 모든 추정값을 [0,1]사이의 확률 값으로 변환하는 역할을 한다(식ㅇ ㅣ없네)
- ![[Pasted image 20240320174200.png]]
- 여기서 성공확률과 실패확률이 0.5로 같아지는 경우가 가장 불순도(불확실성)가 높은 상태이다

4) 로지스틱 회귀모형 해석하기
- X가 성별(남 1, 여 0)을 나타내는 독립변수이고 Y가 구매여부(구매 1 비구매 0)를 나타내는 종속변수라고 할 때, 구매여부에 대한 로지스틱 회귀모형을 실시한다고 하다
- 로지스틱 회귀 모형(읽고 넘김)
- 죽, 로지스틱 회귀분석에서 회귀계수가 매우 작게 추정될경우 독립변수와 관계없이 상수인 확률 0.5를 가짐. 이를 통해 회귀계수가 매우 작은 경우(=회귀계수의 효과가 없는 경우)는 독립변수 X가 Y를 설명하기에 충분하지 않으므로 불확실성이 가장 높은 상태가 된다

다. 임계값(Classification Treshold)
- 로지스틱 회귀분석 결과로 나오는 분류확률이 특정 수준보다 큰지 혹은 작은지를 기준으로 해당 데이터가 어떤 클래스에 속할지 분류할 수 있다. 이 때 기준이 되는 값을 임계값이라고 한다
- 일반적으로 분류를 위한 임계값은 0.5이며 성공확률이 0.5보다 크면 성공 , 보다 작으면 실패라고 분류한다. 하지만 필요에 따라 임계값을 변경하여 분류문제를 해결할 수도 있다.ex 암 진단의 졍우 만약의 경우를 대비해 0.3이나 0.4로 낮추어 민감도를 높일 수 있다. 전체적으로 오분류가 많아지더라도 실제 암환자를 놓치는 사례는 적어질 수 있기 때문

### 의사결정나무
가. 개념
- 의사결정나무는 분류함수를 의사결정 규칙으로 이루어진 나무모양으로 그리는 방법. 계산결과가 의사결정나무에 직접 나타나기 때문에 해석이 간편함
- 의사결정나무는 주어진 입력값에 대하여 출력값을 예측하는 모형으로 분류나무와 회귀나무 모형이 있다.

나. 분석과정
1) 성장 단계
- 각 마디에서 적절한 최적의 분리규칙을 찾아서 나무를 성장시키는 과정으로 적절한 정지규칙을 만족하면 중단
- 정지 규칙은 더이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙. 의사결정나무의 깊이를 지정하거나 끝 마디의 레코드 수의 최소 개수를 지정함
- 분리 규칙을 설정하는 분리기준은 이산형 목표변수, 연속형 목표변수에 따라 나뉜다

	가) 이산형 목표변수
	- 분류나무 : 목표변수가 이산형인 분류나무의 경우 상위 노드에서 가지분할을 할 때 분류변수와 분류 기준값의 선택 방법으로 아래가 사용된다
	- 

| 기준값         | 분리기준                                         |
| ----------- | -------------------------------------------- |
| 카이제곱 통계량 p값 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성      |
| 지니지수        | 지니지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택   |
| 엔트로피 지수     | 엔트로피 지수가 가장 작은 예측변수와 이 때의 최적분리에 의해서 자식마디를 형성 |
	나) 연속형 목표변수
	- 회귀나무 : 목표변수가 연속형인 회귀나무의 경우 분류변수와 분류기준값의 선택방법으로 F-통계량 값, 분산의 감소량 등이 사용됨

| 기준값          | 분리기준                                   |
| ------------ | -------------------------------------- |
| 분산분석에서 F-통계량 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해 자식마디를 형성 |
| 분산의 감소량      | 분산의 감소량을 최대화하는 기준의 최적분리에 의해 자식마디를 형성   |

2) 가지치기 단계
- 오차를 크게 할 위험이 높거나 부적절한 추론규칙을 가지고 있는 가지 또는 불필요한 가지를 제거하는 단계
- 나무의 크기를 모형의 복잡도로 볼 수 있으며, 최적의 나무 크기는 자료로부터 추정하게 된다. 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정 수 이하 일 때 분할을 정지하고 비용-복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 된다

3) 타당성 평가 단계
- 이익도표, 위험더표 혹은 시험용 데이터를 이용하여 의사결정나무를 평가하는 단계

4) 해석 및 예측 단계
- 구축된 나무모형을 해석하고 예측모형을 설정한 후, 예측에 적용하는 단계

다. 의사결정나무 알고리즘
1) CART
- 가장 많이 활용되는 의사결정나무 알고리즘으로 불순도의 측도를 출력(목적)변수가 범주형일 경우 지니지수를 이용, 연속형인 경우 분산을 이용한 이진분리를 사용함
- CART 알고리즘은 이진트리구조로 모형을 형성하는데 목표변수를 가장 잘 분리하는 설명변수와 그 분리시점을 찾는 것이다. 이 척도의 하나가 다양성으로 노드의 다양성을 가장 많이 줄이는 설명변수를 선택한다. 그리고 분리기준은 다음 값을 가장 크게 하는 곳을 선택한다
- 즉, 분리전 다양성 - (왼쪽 자식마디의 다양성 + 오른쪽 자식마디의 다양성) 을 크게 하는 곳을 분리기준으로 정한다
- 개별 입력변수 뿐만 아니라 입력변수들의 선형결합들 중에서 최적의 분리를 찾을 수 있다

2) C4.5와 C5.0
- CART는 이진분리를 하지만 C4.5는 각 마디에서 다지분리가 가능하다 연속변수에 대해서는 CART와 비슷한 방법을 사용하지만 범주형에서는 범주의 수만큼 분리가 일어난다 만약 "색깔"이라는 분리변수로 선택되면 나무의 다른 레벨은 각 색깔별로 노드를 형성함
- C4.5의 가지치기는 Training dataset과 멀리 떨어져있는 데이터에 대해서는 언급하지 않고 가지치기를 한다
- C5.0은 C4.5의 상향 버전

3) CHAID(CHi-squared Automatic Interaction Detection)
- 가장 오래된 알고리즘, SPSS나 SAS통계 패키지에 가장 보편적인 프로그램. 이 알고리즘의 기원은 두 변수간의 통계적 관례를 찾는 것
- 카이제곱 X^2검정(이산형 목표변수) 또는 F-검정(연속형 목표변수)을 이용하여 다지분리를 수행하는 알고리즘. 목표변수가 이산형일 때, p-value값이 가장 작은 예측변수와 그 때의 최적 분리에 의해서 자식마디를 형성시킨다
- CART와 다른 점은 데이터를 과대적합하기 전에 나무 형성을 멈춘다는 것

라. 의사결정나무 장단점

| 장점                                                                                                                                                                                       | 단점                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| - 구조 단순. 직관적 이해. 해석 용이<br>- 유용한 입력변수 파악 가능, 예측변수 상호작용 및 비선형성을 고려하여 분석 가능<br>- 선형성, 정규성, 등분산성 등 통계적 가정이 불필요한 비모수 모형<br>- 낮은 계산 비용 > 대규모 데이터셋도 <br>비교적 빠른 연산 가능<br>- 수치형 / 범주형 변수 모두 사용 가능 | - 분류 기준값의 경계선 부근 자료값은 오차가 큼<br>- 로지스틱 회귀와 같이 각 예측변수의 효과를 파악하기 어려움<br>- 새로운 자료에 대한 예측이 불안정 |

### 서포트 벡터 머신(SVM, Support Vector Machine)
*원리, 주요 용어*
- 패턴인식, 자료분석 등을 위한 지도학습 머신러닝 모델이며, 주로 회귀와 분류 문제 해결에 사용된다
- 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어떤 범주에 속할 것인지를 판단하는 비확률적 이진 선형 분류 모델을 생성한다
- 기존 분류기가 오류율 최소화를 특징으로 한다면 SVM은 마진 최대화로 일반화 능력의 극대화를 추구하며, 마진이 가장 큰 초평면을 분류기로 사용할 때, 새로운 자료에 대한 오분류가 가장 낮아진다

가. 원리
- 데이터가 표현된 공간에서 분류를 위한 경계를 정의한다. 즉 분류되지 않은 새로운 값이 입력되면 경계의 어느 쪽에 속하는 지를 확인하여 분류 과제를 수행한다
- 원과 사각형을 구분 짓는 경계선은 아주 많이 생성될 수 있다. 수많은 경계선 중 SVM은 두 집단에 속한 각 데이터들 사이에서 가장 큰 폭을 가진 경계를 찾는다
- 데이터의 각 그룹을 구분하는 분류자를 결정 초평면이라고 한다. 각 그룹에 속한 데이터들 중에서도 초평면에 가장 가까이에 붙어있는 최전방 데이터들이 결정 경계를 지지하기 때문에 이 점들을 서포트벡터라 한다. 서포트 벡터와 초평면 사이의 수직 거리는 마진이라고 한다
- - SVM은 이와 같이 고차원 혹은무한 차원의 공간에서 마진을 최대화하는 초평면을 찾아 회귀를 수행한다. MMH, 최대 마진 초평면
- ![[Pasted image 20240321124058.png]]

// 알아두기. 초평면이란?
데이터가 존재하는 n차원의 공간보다 한 차원이 낮은 n-1차원의 하위공간을 의미함
즉 3차원 공간에서 초평면은 면이며, 2차원 공간에서 초평면은 선이 된다

나. 적절한 마진의 선책
- 두 클래스로 구분된 데이터에 자신의 클래스가 아닌 다른 클래스 쪽에 가깝게 위치한 데이터(이상치)가 존재할 경우, SVM은 어떻게 분류 문제를 해결할까? 이런 경우에는 분류 경계면을 찾을 때 약간의 오류를 허용하기 위한 파라미타 cost(C)를 활용한다
- C는 데이터가 다른 클래스에 놓이는 것을 허용할 정도를 결정한다. C값을 작게 설정하면 이상치가 존재할 가능성을 많이 허용하여 더욱 일반적인 경계면을 찾고, 반대로 C값을 크게 설정하면 이상치의 존재 가능성을 작게 허용하여 더욱 세심한 분류 경계면을 찾는다
- ![[Pasted image 20240321125752.png]]

1) 소프트 마진
- C값을 작게 설정하여 삼각형과 원으로 분류되는 데이터가 마진 안에 포함되는 것을 어느 정도 허용하는 것.
- 모든 관측치를 완벽하게 분류하지 못해 마진 안에 데이터가 포함되고, 하나의 원은클래스가 잘못 분류되기도 했다. 이처럼 소프트 마진에서는 서포트 벡터와 결정 경계 사이의 거리가 멀어져 마진이 큰 일반적인 분류 경계를 생성한다. 하지만 지나치게 많은 이상치의 존재를 허용하면 과소적합 문제가 발생할 수 있다

2) 하드 마진
- B 서포트 벡터 마진은 C값을 크게 설정하여 이상치의존재를 허용하지 않고, 데이터가 정확하게 분류되도록 기준을 까다롭게 세운 모델이다. 이러한 경우를 하드마진이라고 한다. 하드마진인 B모델은 모든 관측치를 완벽하게 분류하기 위해 매우 좁은 마진을 갖게 되었다. 실제 분석을 진행할 때는 모든 관측치를 완벽하게 분류하는 초평명은 존재하지 않을 수 있으며, 개별 학습 데이터들을 모두 분류하고자 이상치를 허용하지 않는 기준으로 결정 경계를 정하면 과대적합의 문제를 발생할 수 있다

다. 커널
- SVM 모형은 선형 분류뿐만 아니라 비선형 분류에도 사용된다. 비선형 데이터의 분류 문제는 입력자료를 다차원 공간으로 매핑하여 해결할 수 있으며, 이 과정에서 커널 함수를 이용해 계산량을 줄이는 기법인 커널 트릭이 사용된다

### 나이브 베이즈 분류(Naive Bayes Classification)
가. 개념
- 데이터에서 변수들에 대한 조건부 독립을 가정하는 알고리즘. 클래스에 대한 사전 정보와 데이터로부터 추출된 정보를 결합하고, 베이즈 정리를 이용하여 어떤 데이터가 특정 클래스에 속하는 지를 분류하는 알고리즘
- 텍스트 분류에서 문서를 여러 범주 중 하나로 판단하는 문제에 대한 솔루션으로 사용

나. 베이즈 정리
- 나이브 베이즈의 알고리즘의 기본이 되는 개념, **두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리**
- 사건 A와 B가 있을 떼, 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 구하고자 한다. 하지만 현재 가지고 있는 정보는 사건 A가 일어난 것을 전제로 한 B의 조건부 확률, A확률, B 확률 뿐이다. 이 때, 원래 구하고자 했던 **사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률**을 구할 수 있다는 것이 베이즈 정리다
- ![[Pasted image 20240322081433.png]]
- pdsterior = prior * likeihood / evidence
- 사후확률 = 사전확률 * 우도 / 관찰값
- // 조건부 확률 문제도 나올 것 같다....

다. 나이브 베이즈 분류의 계산
- 나이브 베이즈 분류는 하나의 속성 값을 기준으로 다른 속성이 독립적이라 전제했을 때 해당 속성값이 클래스 분류에 미치는 영향을 측정한다
- 속성값에 대해 다른 속성이 독립적이라는 가정을 **클래스 조건 독립성**이라 한다
- ![[Pasted image 20240322081847.png]]

//최근 기출개념! 베이지안 네트워크
- 베이지안 네트워크 빌리프 네트워크 또는 방향성 비순 환그래픽이라고도 불림
- 사전지식을 가지고 가설을 세운 후 이를 구조화하여 만드는 것으로 같은 데이터를 보고 모델링을 하더라도 서로 다른 네트워크를 통해 분석이 가능함
- 베이지안 네트워크는 랜덤 변수의 집합과 방향성 비순환 그래프를 통하여 그 집합을 조건부 독립으로 표현하는 확률의 그래픽 모델
- 그래프의 각 마디는 변수를 나타내고, 마디를 연결하는 호는 변수 간의 조건부 의존성을 표현
- 복잡한 결합분포를 사람이 보다 직관적으로 이해하기 쉬움
- ![[Pasted image 20240322082426.png]]
1) C가 어떻게 결정되는 지에 따라 a와 b도 영향을 받음 -> a와 b는 독립이 아님
2) c가 결정된 후에는 a와 b는 추가적으로 영향을 받지 안흠 -> a|c와 b|c는 독립
3) a가 c에 영향을 주고 c가 다시 b에 영향을 줌 -> a와 b는 독립이 아님
4) c가 결정된 후에는 a가 c에 준 영향을 b가 영향을 받지 않음 -> 즉 a|c와 b|c는 독립
5) a와 b가 c에 함께 영향을 줄 수 있지만, c가 결정되지 않을 경우 a와 b의 값을 독립적으로 결정할 수 있음 -> 즉 a와 b는 독립임
6) c가 결정이 되면, a와 b로 인해 c가 영향을 받는 것으로 볼 수 있음 -> a|c와 b|c는 독립

~이를 통해 알 수 있는 사실로 D-separation 방향성을 가진 네트워크의 경우 c에 의해 a,b가 독립인지 확인하기 위해서는 위의 그림에서와 같이 c가 a,b사이 경로에 있는 Tail-Tail 결합이거나 Head-Tail 결합이어야 하고 c가 a,b 사이의 경로상에 있는 Head-Head 결합이거나 이러한 노드의 자손이면 안됨~

### 앙상블
*대표적 기법인 배깅, 부스팅, 랜던포레스트 개념 및 특징*
- 앙상블 기법은 주어진 자료로부터 여러 개의 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측모형을 만드는 방법이다. 학습방법이 가장 불안전한 의사결정나무에 주로 사용한다
- 가장 대표적인 방법에는 **배깅, 부스팅**이 있다. **랜덤포레스트**는 배깅의 개념과 feature의 임의 선택을 결합한 앙상블 기법이다

가. 배깅
- 주어진 자료에서 여러 개의 붓 스트랩을 자료를 생성하고 각 붓스트랩 자료에 예측모형을 만든 후 결합하여 최종 예측 모형을 만드는 방법이다
- 보팅은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해서 최종결과를 선정하는 과정
- ![[Pasted image 20240322083719.png]]
- 최적의 의사결정나무를 구축할 때 가장 어려운 부분이 가지치기이지만 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정나무들을 활용한다
- 훈련자료의 모집단 분포를 오르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없다. 배깅은 이러한 문제를 해결하기 위해 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있다

나. 부스팅
- 예측력이 약한 모형들을 결합하여 강한 예측모형을 만드는 방법으로 오분류 데이터에 가중치를 부여한다. 에이다부스트는 이진분류 문제에서 랜덤분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다(단 가중치의 합은 1)
- 훈련오차를 빨리, 쉽게 줄일 수 있고 배깅에 비해 많은 경우 예측오차가 향상되어 에이다부스트의 성능이 배깅보다 뛰어난 경우가 많다
- 가장 큰 차이는 **순차적**방법이리는 것 !
- ![[Pasted image 20240322085022.png]]

1) Gradient Boosting Machine
- GBM은 오차를 미분한 그라디언트를 줌으로써 모델을 보완하는 방식으로 부스팅에서 가중치 업데이트를 경사하강법을 이용하여 최적화된 결과를 얻는 알고리즘
- 경사하강법 : 손실함수를 정의하고 이 함수의 미분값이 최소가 되도록 하는 방향을 찾아 접근하는 방식이다
- ![[Pasted image 20240322092619.png]]
- 대표적으로 XGBoost, LightGBM이 있다

	가) 프로세스
	1. 초기값을 상수함수를 적용
	2. Loss function을 최소화라는 그라디언트를 구한다
	3. 그리디언트를 베이스 모델의 타겟값으로 사용하여 그라디언트를 고려한 학습을 진행
	4. 학습률을 더해 최종모형을 만든다
	5. 2 ~ 4 과정을 반복한다

	나) 장단점
	
| 장점                                     | 단점                                                                                                                              |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| - 일반적으로 랜덤포레스트보다 성능이 높음<br>- 예측 성능이 높음 | - 시간이 많이 소요되며 하이퍼파라미터 튜닝 필요<br>- 잔차를 최적화할수록 오차는 적으나 <br>과적화될 가능성이 있어 정규화 알고리즘이 필요<br>- 병렬 처리가 지원되지 않아 대용량 데이터 <br>학습에 많은 시간이 필요 |

2) XGBoost
- 그라디언트 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리로 회귀, 분류 문제를 모두 지원하며 성능과 자원 효율이 좋다
- GBM보다 빠르며, 과대적합 방지가 가능한 규제가 포함되어 있다
- 유연성이 좋아 여러 파라미터를 조절하며 최적의 모델을 만들 수 있으며, 다른 알고리즘과 연계 활용성이 좋다

3) LightGBM
- XGBoost는 굉장히 좋은 성능을 보여주지만, 여전히 학습시간이 느라고 하이퍼파라미터가 매우 많다는 단점이 있다
- LightGBM은 기존의 다른 Tree 기반 알고리즘은 Tree 구조가 수평으로 확장하는 level-wise 방식과 달리 trss구조가 수직으로 확장하는 leaf-wise 방식을 채택해 예측 오류 손실을 최소화 하는 방법이다
- XGBoost보다 속도가 빠르고, 대용량 데이터를 다루며 메모리 사용량이 상대적으로 적고 병렬 컴퓨팅 기능을 제공하고 GPU도 지원하는 장점이 있다. 하지만 적은(만 건 이하)의 데이터셋에 적용할 경우 과적합 발생이 쉽다는 단점이 있다

다. 랜덤포레스트
1) 개념
- 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법이다
- 지도학습 알고리즘으로 다수의 의사결정트리를 사용하여 회귀의 경우에는 평균화를 하고, 분류의 경우에는 투표를 통해 보다 정확한 결과를 예측한다
- ![[Pasted image 20240322090709.png]]
- 수천 개의 변수를 통해 변수제거 없이 실행되므로 정확도 측면에서 좋은 성과를 보인다
- 이론적 설명이나 최종 결과에 대한 해석이 어렵다는 단점이 있지만 예측력이 매우 놓은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅과 부스팅과 비슷하거나 좋은 예측력을 보인다

//참고. 붓스트랩
- 주어진 자료에서 단순랜덤 복원추출 방법을 활용하여 동일한 크기의 표본을 여러개 생성하는 샘플링 방법. 붓스트랩을 통해 100개의 샘플을 추출하더라도 **샘플에 한번도 선택되지 않는 원데이터**가 발생할 수 있는데 **전체 샘플을 약 36.8%** 가 이에 해당한다

2) 장단점

| 장점                                                                                    | 단점                                                                      |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| - 간편하고 빠른 학습 및 테스트 알고리즘<br>- 다중 클래스 알고리즘의 특성<br>- 노이즈에 민감하지 않음<br>- 분류 및 회귀 모두에 적용 가능 | - 매개변수를 잘못 설졍하면 과적합이 발생할 수 있음<br>- 메모리 사용량이 많음<br>- 텍스트 데이터에는 잘 작동하지 않음 |

3) 변수 중요도
- 의사결정나무를 기반으로 하는 모델은 변수 중요도 값을 제공한다. 변수 중요도 값은 불순도를 얼마나 감소시키는 지를 통해 측정되는 것이다.
- MeanDecreaseAccuracy, MeanDecreaseGINI 등의 값으로 변수 중요도를 확인. 이런 값은 평균값으로 더 안정적이며 신뢰성이 높다

## 연관분석
### 연관규칙의 개념(Association Rule)
- 연관성 분석은 흔히 장바구니 분석 또는 서열분석이라고 불린다
- 기업의 데이터 베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용한다
- 장바구니 분석 : '장바구니에 무엇이 같이 들어있는지에 대한 분석'
- 서열분석 : 'A를 산 다음에 B를 산다'

### 연관규칙의 형태
- 조건과 반응의 형태로 이루어져있다
- if A then B : 만일 A가 일어나면 B가 일어난다

### 연관규칙의 측도
- 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 함

가. 지지도
- 전체 거래 중 항목 A와 항목 B가 같이 포함될 확률이다. 

나. 신뢰도
- 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률이다. 연관성의 정도를 파악할 수 있다
- 
다. 향상도
- A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가비. 연관규칙 A -> B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다
- ![[Pasted image 20240322133945.png]]

### 연관분석의 장점과 단점

| 장점                                                                                                                                                                                                                 | 단점(->개선방안)                                                                                                                                                                                                                                                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| - 탐색적인 기법으로 조건 반응으로 표현되는 연관성 분석의 결과를 쉽게 이해할 수 있다<br>- 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우, 목적변수가 없으므로 유용하게 활용된다<br>- 사용이 편리한 분석 데이터의 형태로 거래 내용에 대한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 갖는다<br>- 분석을 위한 계산이 간단하다 | - 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어난다<br>- > 유사한 품목을 한 범주로 일반화한다<br>- > 연관 규칙의 신뢰도 하한을 새롭게 정의해 실제로 드물게 관찰되는 의미가 적은 연관규칙 제외<br>- 너무 세분화된 품목으로 연관성 규칙을 찾으면 의미 없는 분석이 될 수도 있다<br>-> 적절히 구분되는 큰 범주로 구분해 전체 분석에 포함시킨 후, 그 결과 중에서 세부적으로 연관규칙을 찾는 작업 수행<br>- 거래량이 적은 품목은 포함된 거래수가 적을 것이고, 규칙 발견 시 제외하기 쉬움<br>- 그 품목이 관련성을 살펴보고자 하는 중요 품목이라면 유사한 품목들과 함께 범주로 구성하는 방법 등을 통해 연관성 규칙 과정에 포함 |

### Apriori 알고리즘
![[Pasted image 20240322180109.png]]
- 최소 지지도보다 큰 지지도 값을 갖는 품목의 집합을 빈발하목집합이라고 한다
- Apriori 알고리즘은 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것임
- 1세대 알고리즘으로 구현과 이해가 쉽다는 장점이 있으나 지지도가 낮은 후보 집합 생성 시 아이템의 개수가 많아지만 계산 복잡도가 증가한다

## 군집분석
*계층적/분할적 군집분석의 종류, 개념, 특징*
### 개요
- 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체 간의 상이성을 규명하는 분석방법
- 군집 분석은 특성에 따라 고객을 여러 개의 배타적인 집단으로 나누는 것이며, 결과는 구체적인 군집 분석 방법에 따라 차이가 나타날 수 있다
- 군집의 개수나 구조에 대한 가정 없이 데이터들 사이의 거리를 기준으로 군집화를 유도하며, 마케팅 조사에서 소비자들의 상품구매행동이나 Life style에 따른 소비자 군을 분류하여 시장 전략 수립 등에 활용한다
- ![[Pasted image 20240322180652.png]]
- ![[Pasted image 20240322180730.png]]

### 특징
가. 요인 분석과 차이점
- 요인 분석은 유사한 변수를 함께 묶어주는 것이 목적

나. 판별 분석과 차이점
- 판별 분석은 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적

### 거리
- 군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 한다

가. 연속형 변수
- 유클리디안 거리 : 데이터간의 유사성을 측정할 때 많이 사용하는 거리. 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않다

- 표준화 거리 : 해당 변수의 표준편차로 척도 변환한 후 유클리디안 거리를 계산하는 방법이다. 표준화하게 되면 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다

- 마할라노비스 거리 : 통계적 개념이 포함된 거리로서 변수들의 산포를 고려하여 이를 표준화한 거리. 두 벡터 사이의 거리를 산포를 의미하는 표본공분산으로 나눠주어야 하며, 그룹에 대한 ㅏ전 지식이 없는 경우에는 표본공분산을 계산할 수 없으므로 사용하기 곤란하다

- 체비셰프 거리 : 두 벡터의 x좌표 차이와 y좌표 차이 중 큰 값을 갖는 거리

- 맨하탄 거리 : 유클리디안 거리와 함께 가장 많이 사용되는 거리로 맨하탄 도시에서 건물에서 건물을 가기 위한 최단거리를 구하기 위해 고안된거리

- 캔버라 거리 : 두 벡처 사이의 차이에 대한 절대값을 두 벡터의 합으로 나눈 값을 모두 더하여 데이터 간 거리를 구하는 방식

- 민코우스키 거리 : 맨하탄 거리와 유클리디안 거리를 한번에 표현한 공식으로 L1거리(맨하탄 거리), L2거리(유클리디안 거리)라 불리고 있다

나. 범주형 변수
- 자카드 거리 : 1에서 자카트 계수를 뺀 값

- 자카드 유사도 : 자카드 계수라고도 하며, 두 집합 간의 유사도를 측정하는 방법. 자카드 계수는 0과 1 사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 하나도 없으면 0
- 유사도는 값이 클수록 유사하다는 것을 의미하고, 거리는 닥을수록 유사하다는 것을 의미

- 코사인 유사도 : 문서를 유사도 기준으로 분류 혹은 그룹핑 시 유용
- 코사인 거리 : 두 개체의 백터 내적의 코사인 값을 이용해 측정된 벡터간의 유사한 정도

### 계층적 군집분석
- n개의 군집으로 시작해 점차 군집의 개수를 중여나가는 방법
- 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있다
![[Pasted image 20240322181920.png]]

### 계층적 군집분석 종류
가. 최단 연결법
- n x n 거리행렬에서 가장 가가운 데이터를 묶어서 군집 형성
- 군집과 군집 또는 데이터와의 거리를 계산 시 최단거리를 거리로 계산하여 거리행렬 수정 진행
- 수정된 거리행렬에서 거리가 가까운 데이터 또는 군집을 새로운 군집으로 형성


나. 최장연결법
- 군집과 군집 또는 데이터와의 거리를 계산할 때 최장거리를 거리로 계산하여 거리 행렬을 수정하는 방법

다. 평균연결법
- 군집과 군집 또는 데이터와의 거리를 계산할 때 평균을 거리로 계산하여 거리 행렬을 수정하는 방법

라. 와드연결법
- 군집내 편차들의 제곱합을 고려한 방법. 군집 간 정보의 손실을 최소화하기 위해 군집화 진행

마. 군집화
- 거리행렬을 통해 가장 가까운 거리의 객체들 간의 관계를 규명하고 덴드로그램을 그림
- 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수 선정
- 군집 수는 분석 목적에 따라 선정할 수 있지만 대부분 5개 이상의 군집은 잘 안쓴다
- 군집화 단계
	1) 거리행렬을 기준으로 덴드로그램을 그린다
	2) 덴드로그램의 최상단부터 세로축의 개수에 따라 가로선을 그어 군집 개수 선택
	3) 각 객체들의 구성을 고려해서 적절한 군집수 선정
	4) ![[Pasted image 20240322182210.png]]

![[Pasted image 20240322182655.png]]


### 비계층적 군집분석
- n개 개체를 g개 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것

가. k-평균 군집분석(k-means clustering)
1) 개념
- 주어진 데이터를 k개의 클러스터로 묶는 알고리즘. 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작

2) 과정
- 원하는 군집의 개수와 초기 값(seed)들을 정해 seed 중심으로 군집을 형성
- 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류
- 각 군집의 seed 값을 다시 계산
- 모든 개체가 군집으로 할당될 때까지 위 과정 반복
- ![[Pasted image 20240322183104.png]]
- ![[Pasted image 20240322183113.png]]

3) K-평균 군집분석에서 최적의 k 찾기
	가) Elbow 방법
	- 군집분석에서 군집 수를 결정하는 방법으로 군집 수에 따라 군집 내 총 제곱합을 플로팅하여 팔꿈치의 위치를 일반적으로 적절한 군집 수로 선택하는 방법
	- k-means clustering은 클러스터 내 오차 제곱합(SSE)의 값이 최소가 되도록 클러스터의 중심을 결정해 나가는 방법. 클러스터의 개수를 1로 두고 계산한 SSE값과 2로 두고 계산한 SSE값이 더 작다면, 1개의 클러스터 보다 2개의 클러스터가 더 적합하다는 것을 알 수 있다
	- ![[Pasted image 20240322183426.png]]
	- 위의 그림처럼 x축의 클러스터 개수를 늘려가며 계산한 SSE를 y축으로 하여 그래프를 그린다. SSE값이 점점 줄어들다가 어느 순간 줄어드는 비율이 급격하게 작아지는 부분이 생긴다. 그래프 모양을 보면 팔굼치에 해당하는 부분이 우리가 구하려는 최적의 클러스터 개수가 된다. 

	나) 실루엣 기법
	- 실루엣 계수를 사용한 클러스터링의 품질을 정량적으로 계산하는 방법
	- 실루엣 계수는 한 클러스터 안에서 데이터들이 다른 클러스터와 비교해 얼마나 비슷한가를 나타내는 것으로 -1과 1 사이의 값을 가진다
	- 응집도와 분리도가 같으면 실루엣 계수는 0이 되고 이는 데이터들을 클러스터로 분리하는것이 무의미하다는 것을 의미한다
	- ![[Pasted image 20240322183826.png]]
	- 빨간선은 전체 실루엣 계수의 평균으로, 이 값과 각 그룹의 평균 실루엣 계수가 유사해야 군집화가 잘 되었다고 판단

	다) k-평균 군집분석의 특징
	- 거리 계산을 통해 군집화가 이루어지므로 연속형 변수에 활용 가능
	- k개의 초기 중심값은 임의로 선택이 가능하며 가급적이면 멀리 떨어지는 것이 바람직 하다
	- 초기 중감값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집 혼합 되지 않고 층으로 나누어질 수 있어 주의. 초기 중심값의 선정에 따라 결과가 달라질 수 있다
	- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적(greedy)알고리즘이므로 안정된 군집은 보장하나 최적이라는 보장은 없다

| 장점                                                                                                                                       | 단점                                                                                                                                                   |
| ---------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| - 알고리즘 단순, 빠른 수행으로 분석 방법 적용 용이<br>- 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다<br>- 내부 구조에 대한 사전정보가 없어도 의미있는 자료 구조를 찾을 수 있다<br>- 다양한 형태의 데이터에 적용 가능 | - 군집 수, 가중치와 거리 정의가 어렵다<br>- 사전에 주어진 목적이 없으므로 결과 해석이 어렵다<br>- 잡음이나 이상값의 영향을 많이 받는다<br>- 불록한 형태가 아닌 군집(ex. U형태)이 존재할 경우에는 성능이 떨어짐<br>- 초기 군집수 결정에 어려움 |

나. 혼합분포 군집
1) 개요
- 모형 기반의 군집 방법, 데이터가 k개의 모수적 모형(흔히 정규분포 또는 다변량 정규분포를 가정함)의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법 사용
- k개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형 중 어느 모형으로 부터 나왓을 확률이 높은지에 따라 군집의 분류가 이루어진다
- 흔히 혼합모형에서의 모수와 가중치의 추정(최대 가능도 추정)에는 EM알고리즘이 사용된다

2) 혼합 분포 모형으로 설명할 수 있는 데이터의 형태
![[Pasted image 20240322184748.png]]
- 1은 자료의 분포 형태가 다봉형의 형태를 띄므로 단일 분포로의 적합은 적절하지 않으며, 대략 3개 정도의 정규분포 결합을 통해 설명될 수 있을 것
- 2의 경우에도 여러개의 이변량 정규분포의 결합을 통해 설명될 수 있을 것. 두 경우 모두 반드시 정규분포로 제한할 필요는 없다

3) EM(Expectation-Maximization) 알고리즘의 진행과정
- 각 자료에 대해 Z의 조건부분포(어느 집단에 속할지에 대한)로 부터 조건부 기댓값을 구할 수 있다
- 관측변수 X와 잠재변수 Z를 포함하는 (X,Z)에 대한 로그-가능도 함수에 Z대신 상수값인 Z의 조건부 기대값을 대입하면, 로그-가능도 함수를 최대로 하는 모수를 쉽게 찾을 수 있다. (M-단계) 갱신된 모수 추정치에 대해 위 과정을 반복한다면 수렴하는 값을 얻게 되고, 이는 최대 가능도 추정치로 사용될 수 있다

4) 혼합 분포 군집모형의 특징
- k-평균군집의 절차와 유사하지만 확률분포를 도입하여 군집을 수행
- 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다
- EM알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있다
- 군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있다
- k-평균군집과 같이 이상치 자료에 민감하므로 사전에 조치가 필요하다

다. SOM(Self-Organizing Map)
1) 개요
- 코호넨에 의해 제시, 개발되었으며 코호넨 맵이라고도 함
![[Pasted image 20240322185810.png]]
2) 구성
	가) 입력층(입력벡터를 받는 층)
	- 입력 변수의 개수와 동일하게 뉴런 수가 존재
	- 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도라 부름
	- 입력층에 있는 각각의 뉴런은 경쟁층에 있는 각각의 뉴런들과 연결되어 있으며, 이 때 완전 연결되어 있다

	나) 경쟁층(2차원 격차로 구성된 층)
	- 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
	- SOM은 경쟁학습으로 각가의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도를 반복적으로 재조정하여 학습한다. 이 과정을 거치면서 연결 강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 된다
	- 슬자 독식 구조로 인해 경쟁층에ㅓ는 승자 뉴런만이 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열된다

3) 특징
- 고차원 데이터를 저차원의 지도형태로 형상화 > 시각적 이해 쉬움
- 입력 변수의 위치 관계를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가깝게 표현됨. 이런 특징 때문에 팬텬 발견, 이미지 분석 등에서 뛰어난 성능을 보인다
- 역전파(Back Propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 정방 패스를 사용함으로써 속도가 매우 빠르다. 따라서 실시간 학습처리를 할 수 있는 모형이다

4) SOM vs 신경망 모형 

| 구분          | 신경망 모형        | SOM      |
| ----------- | ------------- | -------- |
| 학습방법        | 오차역전파법        | 경쟁학습방법   |
| 구성          | 입력층, 은닉층, 출력층 | 입력층, 경쟁층 |
| 기계학습 방법의 분류 | 지도학습          | 비지도학습    |
