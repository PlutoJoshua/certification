## 딥러닝의 개요
### 딥러닝 개요
- 인공신경망에 기반을 둔 머신러닝의 한 종류로 연속된 층에서 점진적으로 의미있는 표현을 배우는 데 강점이 있으며, 데이터로부터 표현을 학습하는 새로운 방식이다. 인공신경망을 여러 겹 쌓은 것이 곧 딥러닝으로 얼마나 많은 층을 사용했는지가 그 모델의 깊이가 된다
- 인공신경망의 기본 구성요소는 다른 머신러닝 알고리즘이 발전되어서 만들어진 것인데, 머신러닝이 정형 데이터에 적합한데 비해 딥러닝에 잘 맞는 데이터는 비정형 데이터로 보통 '인지'와 관련된 문제를 잘 해결한다고 알려져 있다
- ![[Pasted image 20240322190805.png]]
- 딥러닝은 최근 음성인식과 이미지 인식, 자연어 처리, 헬스케이 등의 전반적인 분야에서 활용되고 있다. 딥러닝 알고리즘으로 학습된 표현의 위의 그림같이 나타난다. 몇 개의 층으로 이루어진 네트워크가 이미지 안의 숫자를 인식하기 위해 Layer를 거쳐 이미지를 변환하여 인식하고 최종적으로 정보를 출력하는 것을 확인할 수 있다
- 딥러닝 알고리즘은 가장 간다난 인공신경망부터 시작되었지만, 크게 인공신경망의 한계를 극복하기 위해 심층 신경망 기법이 제안되었다. 대표적인 심층 신경망의 방법론은 DNN, CNN, RNN, GAN, RB, DBN등이 있다

## 인공신경망(ANN, Artificial Neural Network)
*딥러닝 구조, 뉴런, 활성화 함수 개념, 모형 구축 시 고려사항 숙지*
### 개요
- 인간 뇌의 피질영역 내에는 수많은 뉴런들이 시냅스로 층층이 연결되어 존재하는데, 그것을 신경망이라고 부른다. 인간이 뇌는 100억개의 뉴런과 6조개의 시냅스의 결합체이다
- 인공신경망은 인간 뇌의 신경망에 착안하여 구현된 컴퓨팅 시스템의 총칭이다. 

### ANN의 연구
- 1934년 매컬럭과 피츠 : 인간 뇌를 수많은 신경세포가 연결된 하나의 디지털 네트워크 모형으로 간주하고 신경세포의 신호처리 과정을 모형화하여 단순 패턴분류 모형을 개발
- 헵 : 신경세포 사이의 연결강도를 조정하여 학습 규칙 개발
- 로젠블럿 : 퍼셉트론 인공세포 개발
- **비선형성의 한계점 발생** : **XOR** (Exciusive OR) 문제를 풀지 못하는 한계 발견
- 홉필드, 러멜하트, 맥클랜드 : 역전파 알고리즘을 활용하여 비선형성을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형 등장

### 구조
- 입력층, 은닉층, 출력층으로 구성되어 있으며 입력층과 출력층 사이에 은닉층이 하나인 경우 = 단층신경망, 여러 개인 경우 = 다층 신경망
- 은닉층이 여러개일 때는 각 ㅡㅇ마다 여러 개의 뉴런들로 구성되어 있고, 또 각 층에 있는 뉴런들은 전, 후 층의 뉴런들과 연결되어 있다. 이때 같은 층의 뉴런들은 서로 연결되어 있지 않다
- ![[Pasted image 20240322192013.png]]
- 인공신경망에서 뉴런은 기본적인 정보처리 단위고, 뉴런 여러 개가 가중된 링크로 연결된 형태다. 각 가중된 링크에는 수치적인 가중치가 있고, 이는 인간 뇌의 시냅스에 해당한다
- 가중치는 한 층의 뉴런들이 다른 층은 뉴런들과 연결될 때 그 연결 강도를 결정한다. 다라서 뉴런 간의 가중치가 크다면 서로 강하게 연결되어 있는 것이고 작으면 약하게 연결되어 있는 것이다.
- 최초 가중치는 랜덤 값으로 설정되기 때문에 이 상태에서 어떤 값을 입력해도 원하는 값이 출려되지 않는다. 따라서 인공신경망은 훈련 데이터를 통해 주어진 환경에 적응할 수 있도록 가중치를 반복적으로 갱신하여 신경망의 구조를 선택하고, 활용할 학습 알고리즘을 결정한 후 훈련을 수행한다
- ![[Pasted image 20240322192319.png]]
- 뉴런은 입력링크에서 여러 신호를 받는데 개별신호의 강도에 따라 가중되며, 활성함수는 새로운 활성화 수준을 계산하여 출력 링크로 출력 신호를 보낸다
- 이때 입력 신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있으며, 출력 신호 또한 문제의 최종적인 solution이 되거나 다른 뉴런의 입력이 될 수 있다

### 뉴런의 계산
- 뉴런은 전이함수, 즉 활성화 함수를 사용하며, 활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합을 계산하여 임계값과 비교한다
- 가중치 합이 임계값보다 작으면 뉴런의 출력은 -1, 같거나 크면 +1을 출력함


### 뉴런의 활성화 함수
- 시그모이드 함수의 경우 로지스틱 회귀분석과 유사하며, 0~1의 확률값을 가진다
- ![[Pasted image 20240322192625.png]]
- softmax 함수 : 표준화지수 함수로도 불리며, 출력값이 여러 개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수
- ReLU함수 : 입력값이 0 이하는 0, 0보다 크면 그 x값을 그대로 반환하는 함수. 최근 딥러닝에서 많이 활용하는 함수
- Leaky Relu 함수 : dead ReLU 문제를 해결하기 위해 나온 대안책. dead ReLU란 ReLU모델이 학습하는 동안 일부 뉴런이 0만을 출력하여 활성화되지 않는 문제로, 신경망에서 뉴런의 가중치가 업데이트되다가 음수가 되는 순간 ReLU에 의해 0으로 변환되어 출력되기 때문에 발생함. Leaky ReLU는 이러한 문제를 해결하기 위해 입력값이 0보다 작은 경우 매우 작은 기울기를 부여하여 출력값이 0이 되는 것을 막는ek. Leaky ReLU는 ReLU의 변형된 함수이다
- 하이퍼볼릭 탄젠트(Hyperbolic tangent)함수 : 입력값을 -1과 1사이의 값으로 변환해주는 함수. 시그모이드 함수와는 달리 0을 중심으로 하고 있는데 이 때문에 시그모이드 함수와 비교하면 반환값의 변화폭이 더 크며, 시그모이드 함수보다는 기울기 소실 문제가 적은 편이다

### 단일 뉴런의 학습(단층 퍼셉트론)
- 퍼셉트론은 선형결합기와 하드 리미터로 구성되며, 초평면은 n차원 공간의 두 개의 영역으로 나눈다
- 초평면을 선형 분리 함수로 정의한다

### 신경망 모형 구축시 고려사항
가. 입력변수
- 신경망 모형은 그 복잡성으로 인하여 입력 자료의 선택에 매우 민감함
- 입력변수가 범주형 또는 연속형 변수일 때 아래의 조건이 신경망 모형에 적합함
```
범주형 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주의 빈도가 일정할 때
연속형 변수 : 입력변수 값들의 범위가 변수간의 큰 차이가 없을 때
```
- 연속형 변수의 경우 그 분포가 평균을 중심으로 대칭이 아니면 좋지 않은 결과를 도출하기 때문에 아래와 같은 방법을 활용함
```
변환 : 고객 소득(대부분 평균 미만이고 특정 고객의 소득이 매우 큰)-> 로그변환
범주화 : 각 범주의 빈도가 비스하게 되도록 설정
```
- 범주형 변수의 경우 가변수화(남녀 1, 0 도는 1, -1)하고 가능하면 모든 범주형 변수는 같은 범위를 갖도록 가변수화하는 것이 좋다

나. 가중치 초기값과 다중 최소값 문제
- 역전파 알고리즘은 초기값에 따라 결과가 많이 달라지므로 초기값의 선택은 매우 중요한 문제
- 가중치가 0이면 시그모이드 함수는 선형이 되고 신경망 모형은 근사적으로 선형 모형이 된다
- 일반적으로 초기값은 0 근처로 랜덤하게 선택하므로 초기 모형은 선형 모형에 가깝고. 가중치 값이 증가할수록 비선형 모형이 된다
- 초기값이 0이면 반복하여도 값이 전혀 변하지 않고, 너무 크면 좋지 않은 솔루션을 주는 문제점을내포하고 있어 주의 필요

다. 학습모드
1) 온라인 학습모드
- 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀜
- 일반적으로 속도가 빠르며, 특히 훈련자료에 유사값이 많은 경우 그 차이가 두드러짐
- 훈련자료가 비정상성과 같이 특이선 성질을 가진 경우가 좋다
- 국소최솟값에서 벗어나기 쉬움

2) 확률적 학습모드
- 온라인 학습 모드와 같으나 신경망에 투입되는 관측값의 순서가 랜덤함

3) 배치 학습모드
- 전체 훈련 데이터를 동시에 신경망에 투입함

라. 은닉층(Hidden layer)과 은닉노드(Hidden node)의 수
- 신경망을 적용할 때 가장 중요한 부분이 모형의 선택(은닉층 수, 은닉 노드 수 결정)
- 은닉층과 은닉노드가 많으면 가중치가 많아져 과대적합 문제 발생
- 은닉층과 은닉노드가 적으면                             과소적합 문제 발생
- 은닉칭 수가 하나인 신경망은 범용 근사자이므로 모든 매끄러운 함수를 근사적으로 표현할 수 있다. 그러므로 가능하면 은닉층은 하나로 선정.
- 은닉 노드의 수는 적절히 큰 값으로 놓고 가중치를 감소시켜 적용하는 것이 좋다

마. 과대적합 문제
- 신경망에서는 많은 가중치를 추정해햐 하므로 과대적합 문제가 빈번함
- 알고리즘의 조기종료와 가중치 감소 기법으로 해결
- 모형이 적합하는 과정에서 검증오차가 증가하기 시작하면 반복을 중지하는 조기 종료 시행
- 선형 모형은 능형괴귀와 유사한 가중치 감소라는 벌점화 기법 활용

바. 경사감소소멸(Gradient descent canishing)문제
- 신경망의 층수(은닉층의 개수)가 늘어나면서 앞 쪽에 있는 은닉층들의 가중치가 제대로 훈련되지 않는 현상
- 이런 한계 극복을 위해 DBN, SAE, CNN 등의 딥러닝 알고리즘이 인공신경망을 기반으로 생겨남

## 심층 신경망(DNN, Deep Neural Network)
### DNN의 개요
*기본적인 개념과 특징 알아두기*
- 심층신경망은 입력층과 출력층 사이에 다중의 은닉층을 포함하는 인공신경망을 뜻함

### 특징
- 심층신경망은 다중의 은닉층을 가지고 있기 때문에 데이터의 잠재적인 구조를 파악할 수 있으며 비선형적 관계를 학습할 수 있다
- 하지만학습을 위해 수많은 연산이 필요하고, 과한 학습으로 인해 실제 데이터에 오히려 오차가 증가하는 과적합 혹은 높은 시간 복잡도 등의 문제가 발생할 수 있다. 2000년대에 들어서면서 드롭아웃, ReLU, 배치 정규화 등의 기법을 적용함으로써 이러한 문제를 해결하여 딥러닝의 핵심 모델로 활용되고 있다
- 의료분야에서 암 진단 시스템 구축 등에 활용되고, 재무분야는 주가지수 예측, 기업신용평가, 환율 예측 등에 활용되고 있다
- 알고리즘에 따른 심층신경망의 종류로는 이미지 데이터를 주로 처리하는 CNN, 시계열 데이터를 처리하는 RNN, 비지도형 기계학습을 기반으로 하는 DBN, 심층 오토인코더 등이 있다
// 알아두기. 최근 기출개념. 오토 인코더
- 오토인코더는 **인코더**를 통해 입력으로 신호를 변환한 다음 다시 **디코더**를 통해 레이블 등을 만들어 내는 비지도 학습 방법
- 보통 **입력층보다 적은 뉴런을 가진 은닉층**을 중간에 넣어 줌으로써 차원을 줄이고 이때 학습을 통해 소실된 데이터를 복원한 후 입력 데이터의 특징을 효율적으로 응축한 새로운 출력을 만들어낸다
- 입력과 출력이 동일한 **좌우대칭** 으로 구축되는 구조이다
- 인코더는 입력을 내부 표현으로 변환하는 역활을 수행하며 인지네트워크, 디코더는 내부 표현을 다시 출력으로 바꾸는 역할을 수행하며 생성 네트워크 라고도 함
- ![[Pasted image 20240322195219.png]]

## 합성곱 신경망(CNN, Convolution neural network)
### CNN의 개요
- 사람의 시각인지 과정을 모방해서 인공신경망에 필터링 기법을 적용한 것으로 이미지와 같은 2차원 데이터를 분석하는 심층신경망 방법론
- CNN은 필터링을 통해 입력된 이미지로부터 특징을 추출한 뒤 신경망에서 분류 작업을 수행한다
- 예를 들어 어떤 이미지를 CNN에 입력시켜주면, 그 이미지가 개인지 고양이인지에 해당하는 이미지 분류를 수행하도록 학습시킬 수 있다
- CNN은 영상인식, 영상 분류, 이미지 인식, 자연어 처리 등에 주로 사용되며, 자율주행자동차, 이밎/텍스트.사운드/비디오 인식 및 식별 등의 연상, 그림 인식 분야 등에서 활용되고 있다

### CNN 알고리즘
- 기존 인공신경망의 경우는이미지 픽셀값들을 그대로 입력받아서 어떤 클래스에 속하는지 분류하는데, 같은 클래스의 데이터지만 살짝만 변형이 생겨도 각 경우에 대한 훈련데이터가 모두 필요했고 그에 따른 훈련 시간도 상당히 길어진다는 단점이 있었다
- 그래서 CNN알고리즘에서는 이미지 픽셀값들을 그대로 입력받는 것보다는 이미지를 대표할 수 있는 특성들을 도출해서 신경망에 넣어주는 방법을 사용하게 되었다. CNN 알고리즘은 특징을 추출하는 합성곱 레이어(컨볼루션 레이어)와 추출된 특징 데이터의 사이즈를 줄이고 노이즈를 상쇄시키는 '풀링레이어'로 구성된다
![[Pasted image 20240322200347.png]]

가. 합성곱 과정
- 이미지 데이터가 입력되었을 때 특징을 뽑아내는 과정에 해당함. 합성곱 층의 뉴런은 이미지의 모든 픽셀에 연결되지 않고 수용역역 안에 있는 픽셀에만 연결된다. 수용역역의 용량만큼 필터가 존재하며, 필터는 커널이라고도 한다
- 필터는 합성곱층에서의 가중치 파라미터에 해당하며, 학습 단계에서 적절한 필터를 찾도록 학습된다. 컨볼루션 또는 필터링 과정을 통해 얻은 특성 지도들은 ReLU와 같은 활성화 함수를 거쳐 출력된다
- 합성곱 연산을 적용하면 입력 데이터보다 작은 크기의 특성 맵이 출력되며, 이 과정이 반복되면 데이터의 크기가 작아지고 정보 손실이 발생할 수 있다. 이러한 문제를 해결하기 위해 이미지의 가장자리를 특정값으로 감싸는 패딩 과정을 수행할 수 있다

1) Feaure Map
- CNN에서합성곱 계층의 입출력 데이터를 Feature Map 이라고 한다. 입력된 Feature Map은 Filter를 통해 출력 Feature Map으로 나오게 된다
- Filter는 주로 4 x 4 , 3 x 3의 행렬로 정의되며, 특징을 추출하기 위해 사용하는 파라미터이다. 지정된 간격으로 이동하면서 전체 입력 데이터와 합성곱을 통해 Feature Map을 만들어 낸다. 여기서 지정된간격으로 필터를 순회하는 간격을 Stride 라고 한다. Filter와 Stride에 따라 결과 값이달라질 수 있기 때문에 알고리즘 처리에서 중요한 역할을 한다
- ![[Pasted image 20240322201244.png]]
- ![[Pasted image 20240322201258.png]]
- ![[Pasted image 20240322201311.png]]![[Pasted image 20240322201349.png]]![[Pasted image 20240322201400.png]]
- ![[Pasted image 20240322201430.png]]![[Pasted image 20240322201449.png]]
- 1도 이해 안감

2) 패딩
- 출력 Feature Map를 계산할 때, Stride 설정에 따라 출력 Feature Map이 정수로 나누어 떨어지지ㅣ않아 필터나 스트라이드를 사용할 수 없게 된다. 이런 문제로 정보가 사라지는 등의 문제가 발생할 수 있어 Padding을 을 사용하여 방지한다
- 패딩은 입력 특성 맵의 외곽에 지정된 픽셀만큼 특정값으로 채워넣는 것을 의미한다. 보통 패딩 값으로 0을 채워 넣는다
- ![[Pasted image 20240322201729.png]]

나. 풀링 과정
- 합성곱 연산 이후에 풀링이라고 불리는 과정을 통해 활성화된 특성 지도들의 크기를 줄인다. 이 특성 지도들에 다시 컨볼루션. 활성화, 서브 샘플링을 수행해서 점차적으로 로컬한 특성지도로부터 글로벌한 특성 지도를 만들어간다
- 이 과정을 여러 번 반복하여 얻어진 최종 특성지도는인공신경망에 입력되어 이미지가 어떤 클래스 라벨에 속하는지 분류해준다. 그래서 CNN은 특성 추출 신경망과 분류 신경망을 직렬로 연결한 구조로 되어 있다고 말할 수 있다
- 풀링은 합성곱 연산 이후에 필수적으로 적용해야 하는 것은 아니며, 데이터의 사이즈를 줄이고자 할 때 선택적으로 적용하는 과정임
- ![[Pasted image 20240322202030.png]]
- 평균풀링은 평균값으로 대표값을 뽑아내는 거!
- https://bigdaheta.tistory.com/48

## 순환 신경망(RMM, Recurrent Neral Network)
### RNN의 개요
- 시계열 데이터를 처리하기 위한 모델로 순차적이며 반복적인 데이터를 학습하는데 특화된 알고리즘
- 내부가 순환구조로 이루어져 있는 인공신경망. 입력층, 은닉층, 출력층 3단계 구조로 이루어져 있으나 은닉층이 이전 데이터를 참조하도록 서로연결되어 있음
- 순환구조를 통해 과거 학습했던 내용을 현재의 학습에 반영함
- 신경망 내부에 상태를 저장하여 시퀀스 형태의 데이터 입력을 처리하여 앞으로의 데이터를 예측
- ![[Pasted image 20240322202345.png]]
- 동일한 가중치와 편향이 모든 입력값에 대해서 동일하게 사용되며, RNN네트워크를 재귀적으로 사용하므로 길이에 대한 제한이 없다

### RNN의 특징
- RNN은 보통 음성인식이나 텍스트의 앞 뒤 단어를 파악하는 분석과 같이 과거 데이터를 고려하여 현재 입력데이터를 순차적으로 처리하는 분석에 주로 사용되고 자동 번역, 단어 의미 판단, 이미지 캡션 생성 등의 자연어 처리 분야 등에서 활용되고 있다
- 선형 함ㅅ가 아닌 비선형 함수를 활성함수로 쓰는 것과 비슷한 이유로 초기값에 따라서 과거 데이터를 계속 곱할수록 작아지는 문제가 발생한다
- 매 시점마다 심층신경망이 연결되어 있을 경우, 오래 전의 데이터에 대한 기울기 값이 소실되는 문제가 발생할 수 있어 학습이 어려워진다
- 이를 해결하기 위해 LSTM(Long Short term Memory, 장단기 메모리)방식의 순환신경망이 있다

## LSTM(Long Short term Memory)
### 개요
- RNN의 문제점은 입력된 데이터와 참고해햐 할 데이터의 위치 차이가 커질 때 문맥을 연결하기 힘들어진다는 것이었다. 이러한 장기 의존성 문제(은닉층의 과거 정보가 마지막까지 전달되지 못하는 현상)을 보완할 수 있는 딥러닝 프레임 워크이다
- - RNN은 단순히 이전은닉층 값과 현재 입력값에 각각 가중치를 곱하고 tahn 함수를 거쳐 얻은 출력값을 해당 순번의 은닉층 값으로 계산하는 과정을 반복한다. LSTM에서도 은닉층ㅇ-서 출력층으로 넘어가는 연산은 RNN과 동일하다
- 하지만 LSTM에서는 이전 단계의 정보를 메모리 셀에 저장하여 현재 시점의 정보를 토대로 과거 내용을 얼마나 반영할지 결정하고 그 결과에 현재의 정보를 더해서 다음 시점으로 정보를 전달한다
- LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보인다
- LSTM은각각의 메모리와 결과값을 컨트롤 가능하다는 장점이 있고 메모리가 덮어씌워질 가능성과 연산속도가 느리다는 단점이 있다![[Pasted image 20240322203228.png]]
