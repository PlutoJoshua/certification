## 데이터 수집 및 전환

### 데이터 수집
- 조직 내/외부에 분산된 다양한 데이터 원천으로부터 필요한 데이터를 검색하여 수동/자동으로 데이터를 수집하는 단계. 수집 데이터를 저장/분석하기 위해 변환 및 통합 작업을 포함하기도 함

가) 빅데이터 수집 기법 *주요한 빅데이터 수집 기법 명칭, 특징, 대표적인 도구 예시 기억하기*
- 우리 삶을 영위하는 환경이 정보기기를 통해서 스마트해지면서 의료/금융.교육/과학/유통/제조 등 사회 전 분야에서 이전에는 상상하지 못했던 다양한 유형의 데이터가 끊임없이 대량으로 생성되고 있다. 또한 이러한 기기들이 인터넷으로 연결되면서 사람들은 소셜 네트워크 환경에서 새로운 사회적 관계를 맺고 활동하는 일상이 데이터로 그대로 기록되어 있다
- 빅데이터 수집 기법은 포털사이트 및 소셜 네트워크 등의 디지털 가상 공간에서 실시간으로 생성되는 HTML, XML 형태의 모든 데이터를 수집하기 위한 기법이다

1) 정형 데이터 수집 기법  

| 종류    | 주요 특징                                                                                                                                                                                                                        |
| ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ETL   | Extract Transform Load<br>다양한 데이터 원천으로부터 수집한 데이터를 데이터 웨어하우스와 데이터 마트로 원 데이터를 이동시키기 위해 사용하는 추출, 변환, 적재 프로세스 및 기술                                                                                                               |
| FTP   | File Transfer Protocol<br> TCP/IP 프로토콜을 통해 원거리에 있는 서버와 클라이언트 사이의 파일 전송을 위한 프로토콜<br>클라이언트가 데이터를 수신 받을 포트를 먼저 알려주는 Active FTP 방식과 서버가 임의로 알려준 포트에 클라이언트가 접근하는 Passive FTP 방식이 있다                                               |
| API   | Application Programing Interface<br>응용 프로그램에서 사용할 수 있도록, 운영 체제가 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스<br>시스템 간 연동을 통해 실시간으로 데이터를 수신할 수 있는 기능 제공                                                                               |
| Sqoop | - 구조화된 관계형 데이터베이스와 아파치 하둡 간의 대용량 데이터들을 효율적으로 변환하여 주는 명령 줄 인터페이스 애플리케이션<br>- 오라클 또는 MySQL과 같은 관계형 데이터베이스에서 하둡 분산 파일 시스템으로 데이터들을 가져와서 그 데이터들을 하둡 맵리듀스로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있게 하는 기능 제공<br>장애 허용 능력 및 병렬 처리 가능 |

2) 반정형 데이터 수집 기법
- 반정형 데이터 수집 기법으로 가장 많이 활용되는 것은 로그수집기. 이는 끊임없이 생성되는 대량의 데이터를 연속적으로 처리하기에 적합한 도구로 현재 다양한 오픈소스 솔루션이 공개되어 있는데 그중 3개가 가장 많이 활용됨

| 종류     | 주요 특징                                                                                                                                                                                                                                                                                                                                 |
| ------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Scribe | - 페이스북에서 개발한 실시간 스트리밍 로그 데이터 수집 애플리케이션<br>- 분산된 서버에서 발생하는 데이터를 중앙 집중 서버로 전송하는 방식 이용<br>- 네트워크와 시스템의 장애를 해결하기 위해 고안된 애플리케이션으로 확장성과 신뢰성을 목표로 함<br>- 설치 및 구성이 용이<br>- 페이스북은 수천 대를 설치 및 운영함으로써 하루에 100억개 이상의 메세지 수집                                                                                                                       |
| Flume  | - 분산 환경에서의 대량 로그 데이터를 효과적으로 수집하고 합친 후 효율적으로 전송할 수 있는 서비스<br>- 클러스터 환경에서 장애에 쉽게 대처 가능하고 로그 유실에 대한 신뢰 수준을 상황에 맞게 변경함으로써 신뢰성 있는 데이터 수집을 수행<br>- 다양한 장비에서 수집되는 로그 파일 데이터를 하둡 분산 파일 시스템(HDFS)과 같은 중양 저장소에 저장하는 로깅 시스템 구축 시 적합<br>- 핵심 목표는 신뢰성/확정성/운영가능성/가용성<br>- Cloudera에서 채택하여 활용함                                                       |
| Chukwa | - 분산된 노드들의 다양한 로그 데이터를 수집하고 수집된 데이터를 HDFS에 저장하고 분석하기 위한 시스템<br>- 주요 수집 로그는 모니터링 로그, 하둡로그, 응용프로그램 로그 둥이며 테라바이트 단위 이상의 로그데이터를 실시간으로 모니터링 가능함<br> 사용자는 HICC(Hadoop infrastudture Care Center)라는 웹 포털 인터페이스를 통해 하둡 클러스터의 로그 파일과 시스템 정보를 분석하여 웹 UI를 통해 결과를 노출함. 해당 부분은 로그 수집을 위한 용도보다는 하둡 클러스터를 실시간으로 모니터링 하고 관리하는 기능임.<br>- 야후에서 채택하여 활용함 |

3) 비정형 데이터 수집 기법
- 비정형 데이터 수집기법으로는 크롤링, 스크래피, 아파치 카프카, RSS, Open API등이 있음
- 그중 두 가지의 활용도가 크게 증가

| 종류      | 주요 특징                                                                                                                                               |
| ------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| 스크래피    | - Python으로 작성된 오픈 소스 웹 크롤링 프레임 워크로서 웹데이터를 수집<br>- API를 이용하여 데이터를 추출하거나, 범용 웹 크롤러로 사용 가능<br>- 개발자들의 도움으로 코드 재사용성을 향상시킴으로써, 대규모 크롤링 프로젝트 개발이 용이하도록 지원 |
| 아파치 카프카 | - 실시간 데이터 피드를 관리하기 위해 높은 처리량, 낮은 지연시간을 지닌 플랫폼제공<br>- 분산 트랜잭션 로그로 구성. Scale-Out 방식의 수평 확장 가능<br>- 스트리밍 데이터 처리를 위한 기업 인프라를 위한 고부가 가치를 제공함으로써 활용이 증가   |
- 비정형 데이터 수집을 위해 가장 흔히 활용하는 것은 크롤링으로서, 여러 웹페이지를 돌아다니며 데이터를 수집하는 모습. 유사한 의미로 스파이더링이라고도 함
- 크롤링은 주로 인터넷 상의 여러 웹페이지에서 HTML 코드, 문서 등의 데이터를 수집해서 분류하고 저장하는 것. 이를 위해 웹 로봇 및 웹 크롤러 등을 이용함. 크롤링의 주요 목적은 데이터가 어디에 저장되어 있는지 위치에 대한 분류작업임

| 종류    | 주요 특징                                                                                                                                                                                                                                                                  |
| ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 웹 로봇  | - 사람과의 상호작용없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램<br>- 최초로 지정된 URL 리스트에서 출발하여 웹 문서를 수집하고 수집된 웹 문서에 포함된 URL을 자동으로 추출하여 해당 URL에 대한 웹 문서 수집 과정을 계속적으로 반복<br>- 주로 검색엔진에서 활용되며 웹 문서를 서핑하면서 필요한 정보를 수집하고 색인 및 정리 기능을 수행                                                        |
| 웹 크롤러 | - 조직적이고 자동화된 방법으로 월드와이드웹(www)을 탐색하는 컴퓨터 프로그램. 웹을 돌아다니면서 만나는 모든 문서 정보를 수집하기 위해 사용<br>- 검색엔진에서 주로 사용하며 방문한 모든 페이지의 복사본을 생성하고 생성된 페이지에 대해 인덱싱을 수행함으로써 빠른 검색이 가능하도록 함<br>- 웹 페이지의 특정 형태 정보 수집(ex. 자동 이메일 수집 등)에 이용되거나 웹사이트 자동 유지 관리 작업(ex. HTML 코드 검증, Link 체크 등)에 이용되기도 함 |

나. 데이터 소스에 따른 수집 방법
- 데이터 소스의 위치에 다라 내부데이터와 외부데이터로 구분할 수 있으며 해당 부분에 따라 다른 방법으로 데이터를 수집할 수 있다

| 데이터 원천 | 개념                                        | 대표적 수집 방법                                                                                                                               |
| ------ | ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| 내부 데이터 | 자제척으로 보유한 내부 파일 시스템, RDBMS, 센서 등에 접근하여 수집 | ETL 방식<br>-Extraction(추출) : 필요 데이터 추출<br>- Transform(변환) : 분석에 적절한 구조로 데이터를 제장하기 위한 변환<br>- Loading(적재): 목표시스템으로 저장하고자 하는 대상을 추출하여 데이터화 |
| 외부 데이터 | 인터넷으로 연결된 외부 데이터 수집                       | 크롤링<br>- 웹페이지 내용 전체를 수집하여 저장하고자 하는 대상을 추출하여 데이터화                                                                                        |

다. 빅데이터 수집 시스템의 요건
- 빅데이터 수집 시스템은 다양한 데이터 원천으로부터 데이터를 수집하기 위해 확장성/안정성/유연성/실시간성의 요건을 확보해야 함
	1) 확장성 : 데이터 수집의 대상이 되는 서버는 충분한 확장이 가능
	2) 안정성 : 수집된 데이터는 유실, 변경, 삭제되지 않고 안정적으로 저장
	3) 유연성 : 다양한 데이터 원천의 여러 포맷에 적용할 수 있도록 변경이 용이
	4) 실시간성 : 수집된 데이터는 실시간으로 반영

라. 빅데이터 수집 절차

|                                                             |               | ↓----                                                                                                                          | ----          | ----                                                     |
| ----------------------------------------------------------- | ------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------- | -------------------------------------------------------- |
| 수집 대상 선정                                                    |               | 수집 계획 수립                                                                                                                       |               | 수집 실행                                                    |
| - 수집 도메인 도출<br>- 수집 데이터셋 도출<br>- 수집 리스트 작성<br>- 수집 대상 부서 파악 | <br><br><br>> | - 데이터 제공 여부 협의<br>- 데이터 유형 및 속성 확인<br>- 수집 환경 및 표준 파악<br>- 수집 주기 및 용량 파악<br>- 수집 연동 및 포맷 파악<br>- 수집 기술 선정<br>- 수집 정의서 및 계획서 작성 | <br><br><br>> | - 단위 테스트 진행<br>- 연동 테스트 진행<br>- 데이터 수집 실행<br>- 데이터 적재 처리 |

### 데이터 유형 및 속성 파악
- 데이터의 유형 및 속성 등 데이터 고유의 특성을 명확히 인지함으로써 기술적 사항을 고려한 데이터 처리 과정 설계가 가능함

가. 수집 대상에 따른 데이터 유형
- 빅데이터 수집 시스템의 수집 대상이 되는 데이터는 구조, 시간, 저장 형태 관점에 따라 데이터 유형을 구분할 수 있다

|        | 관점    | 데이터 유형     |
| ------ | ----- | ---------- |
|        |       | 정형데이터      |
|        | 구조 관점 | 비정형데이터     |
|        |       | 반정형 데이터    |
|        |       |            |
|        | 시간 관점 | 실시간 데이터    |
| 수집 데이터 |       | 비실시간 데이터   |
|        |       |            |
|        |       | 파일데이터      |
|        | 저장 형태 | 데이터베이스 데이터 |
|        |       | 콘텐츠 데이터    |
|        |       | 스트림 데이터    |

나. 일반적인 데이터의 특징
- 데이터의 형식을 결정하는 존재론적 특징을 기준으로 데이터를 구분하면 정성적 데이터와 정량적 데이터로 구분할 수 있다

| 구분    | 정성적 데이터           | 정량적 데이터             |
| ----- | ----------------- | ------------------- |
| 형태    | 비정형 데이터           | 정형/반정형 데이터          |
| 특징    | 객체 하나에 함의된 정보를 가짐 | 속성이 모여 객체를 이룸       |
| 구성    | 언어, 문자 등으로 이루어짐   | 수치, 도형, 기호 등으로 이루어짐 |
| 저장 형태 | 파일, 웹             | 데이터베이스, 스프레드 시트     |
| 소스 위치 | 외부 시스템(주로 소셜 데이터) | 내부 시스템(주로 RDBMS)    |
// **최근 기출 개념**. 정성적 데이터와 정량적 데이터의 수집 방법

| 유형      | 수집 방법                                   | 특징                                                                 |
| ------- | --------------------------------------- | ------------------------------------------------------------------ |
| 정성적 데이터 | FGI<br>(Focus Group Interview)          | 특정 이슈에 직접적으로 관련된 사람들로 토론 그룹을 형성하여 심층적인 면접 실시                       |
|         | 심층 면접<br>(In-depth Interview)           | 1명의 조사 대상자와 일대일 면접을 통해 심리를 파악하는 조사법                                |
|         | 델파이 조사<br>(Delphi method)               | 특정 주제에 대하여 전문가 집단을 구성하여 이들에게 여러 차례 의견을 수집                          |
|         | 관찰조사/동반쇼핑<br>(Shadowing/Town Wataching) | 백화점, 마트 등에서 소비자가 쇼핑하는 행태를 관찰하거나 지인을 가장하여 쇼핑하며 관찰하는 조사              |
| 정량적 데이터 | 면접조사<br>(Face-to-Face Interview)        | 연구자가 조사대상자와 직접적인 상호작용을 통해 필요한 자료를 수집하여 기록                          |
|         | 전화/우편/Fax/E-Mail 조사                     | 질문자를 조사대상자들에게 보내어 회수                                               |
|         | 온라인 조사                                  | 네트워크, 인터넷 등 컴퓨터가 연결된 상태에서 이루어지는 조사                                 |
|         | CLT조사<br>(Central Location Test)        | 다양한 표본을 조사할 수 있는 상점가 등에서 조사장소를 설치하고 여기에 조사 대상자를 불러 모아 간단하게 조사하는 방법 |
|         | 갱 서베이<br>(Gang Survey)                  | 조사 대상 집단을 일정한 장소에 모아서 조사자의 진행으로 질문지에 응답하도록 하는 방법                   |

다. 구조관점의 데이터 유형 
*다양한 관점을 기준으로 구분한 데이터 유형 기억하기 각 유형의 특징 및 주요 예시들 숙지*
- 데이터를 구조 관점에서 분류

| 유형         | 특성                                                                 | 종류                                                                                                        |
| ---------- | ------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- |
| 정형 데이터     | - 정형화된 스키마를 가짐<br>- 일관성 있는 값고 형식                                   | 관계형 데이터베이스(RDB), 스프레드 시트, 파일, 통계                                                                          |
|            | 데이터 수집 난이도                                                         | 잠재 가치                                                                                                     |
|            | 난이도 *하*<br>내부 시스템인 경우가 대부분이라 수집이 용이                                | 내부 데이터의 특성상 데이터 활용 측면에서 잠재적 가치는 상대적으로 낮은 편                                                                |
| 반정형<br>데이터 | - 정형화된 스키마를 가짐<br>- 값과 형식에서 일관성이 없음<br>- 메타데이터포함                   | XML, HTML, 웹 로그, 알람, 시스템 로그, JSON, RSS, 센서데이터                                                             |
|            | 난이도 *중*<br>보통 API 형태로 제공되기 때문에 데이터 처리 기술이 요구됨                      | 데이터 제공자가 선별해 제공하는 데이터로 잠재적 가치는 정형 데이터보다 높음                                                                |
| 비정형<br>데이터 | - 스키마가 없음                                                          | 소셜미디어, NoSQL, 웹 게시판, 텍스트, 이미지, 오디오, 비디오                                                                   |
|            | 난이도 *상*<br>텍스트 마이닝 혹인 파일일 경우 파일을 데이터 형태로 파싱해야하기 때문에 수집 데이터 처리가 어려움 | 수집주체에 의해 데이터에 대한 분석이 선행되었기 때문에 목적론적 데이터 특징이 가장 잘 나타나는 데이터. 따라서 일단 수집이 가능하다면 수집주체에게는 가장 높은 잠재적 가치를 제공하는 유형 |

1) 정형 데이터(structuerd data)
- 미리 정해 놓은 형식과 구조에 따라 저장되도록 구성된 데이터. = 구조적 데이터
- 데이터의 스키마 정보를 관리하는 DBMS와 데이터 내용이 저장되는 데이터 저장소로 구분됨
- 종류 : RDBMS의 테이블 데이터(단일 테이블 혹인 조인한 테이블 포함), 스프레드 시트 데이터, CSV 데이터 등
- 수집기술 : ETL, FTP, Open API, Sqoop

2) 비정형 데이터(semi-structured data)
- 데이터의 형식과 구조가 변경될 수 있는 데이터. 데이터 내부에 정형 데이터의 스키마에 해당되는 메타데이터를 갖고 있으며 일반적으로 파일 형태로 저장됨
- 메타데이터는 데이터의 구조 정보이므로 데이터가 어떤 형태를 가졌는지 파악하는 것이 필요
- 파일에 포함된 메타데이터를 바탕으로 테이블 형태의 데이터 스키마로 변환되고 데이터를 매핑하여 정형 데이터로 변환할 수 있다. 특히 JSON 데이터는 CSV 또는 테이블 형태로 쉽게 변환되므로 데이터 분석을 위해서는 변환을 거쳐 사용할 수 있다
- 종류 : URL 형태로 존재(HTML), Open API 형태로 제공(XML, JSON), 로그 형태(웹 로그, IoT에서 제공하는 센서 데이터)
- 수집 기술 : Crawling, RSS, FTP, Open API, Flume, Scribe, Chukwa

3) 비정형 데이터(unstructured data)
- 일반적으로 정의된 구조가 없이 정형화되지 않은 데이터
- 수집 대상은 데이터 세트가 아니라 객체화되어 있는 하나의 데이터
- 언어 분석이 가능한 텍스트 데이터나 이미지, 동영상 같은 멀티미디어 데이터가 대표적
- 웹에 존재하는 데이터의 경우 반정형 혹은 비정형 데이터로 명확하게 구분하기 어려울 수 있음(HTML 형태로 존재하지만 텍스트마이닝을 통해 데이터 수집도 가능하기 때문)
- 데이터 분석을 위해서는 데이터의 특징을 추출하여 반정형 또는 정형 데이터로 변환하는 전처리 필요하며 입력 데이터의 종류에 따라 매우 다양한 방법의 전처리를 사용함. 텍스트 데이터는 전처리를 위해 자연어 처리 기법을 주로 사용하고, 미디어 파일은 필터를 이용하여 노이지를 제거하거나 데이터 범위를 변화하는 방법을 사용함
- 종류 : 이진 파일 형태(동영상, 이미지), 스크립트 파일 형태(소셜 데이터의 텍스트)
- 수집 기술 :  Crawling, RSS, FTP, Open API, Streaming, Scrapy, Apache Kafka

라. 시간 관점의 데이터 유형
- 데이터를 활용주기에서 분류

| 유형                   | 특성                                                       | 종류                                      |
| -------------------- | -------------------------------------------------------- | --------------------------------------- |
| 실시간<br>데이터           | 생성된 이후 즉시 처리 혹은 데이터가 처리 유효 시간 내에 처리되어야만 효용 가치가 있는 현재 데이터 | 센서데이터, 시스템 로그, 네트워크 장비 로그, 알람, 보안 장비 로그 |
| 비실시간 데이터<br>(배치 데이터) | 생성된 이후 일정 시간이 지난 후에라도 효용 가치가 있는 과거 데이터                   | 통계, 웹로그, 구매 정보, 서비스 로그, 헬스케이 정보         |

1) 실시간 데이터(realtime data)
- 센서 데이터, 시스템 로그 등과 같이 생성된 이후 수 초 ~ 수 분 이내에 처리되어야 의미가 있는 현재 데이터를 말함. 즉시 처리가 필요한 데이터들

2) 비실기간 데이터(non-realtime data)
- 통계, 웹 로그, 구매 정보 등과 같이 생성된 데이터가 수 시간 또는 수 주 이후에 처리되어야 의미가 있는 과거 데이터를 말함. 축적의 의미가 있는 데이터들

마. 저장 형태 관점의 데이터 유령
- 빅데이터 수집 시스템에서 수집 데이터를 저장하는 형태의 관점에서 분류
1) 파일 데이터
- 시스템 로그, 텍스트 등과 같이 파일 형식으로 파일 시스템에 저장되는 데이터. 파일 크기가 대용량이거나 파일 개수가 다수임

2) 데이터베이스 데이터
- 데이터의 종류나 성격에 따라 관계형 데이터베이스, NoSQL, 인메모리 데이터베이스 등에 데이터베이스의 컬럼 또는 테이블 등에 저장된 데이터

3) 콘첸츠 데이터
- 텍스트/이미지/오디오/비디오 등 개별적으로 데이터 객체로 구분될 수 있는 미디어 데이터

4) 스트림 데이터
- 센서 데이터, HTTP 트랜잭션, 알람 등과 같이 네트워크를 통해 실시간으로 전송되는 데이터

### 데이터 변환
- 일반적으로 데이터 전환은 추출, 변환, 적재의 과정. = ETL
- 과거에는 데이터베이스관리시스템의 쿼리를 이용하거나 별도 소프트웨어를 개발하여 이루어졌으나 현재는 데이터 양의 증가 및 전환 효율성 측면을 고려하여 데이터 전환 도구(WTL)들이 광범위하게 사용되고 있다
- 전환 작업의 난이도/분량/소요시간/비용 등을 고려하여 데이터 전환 도구의 활용 여부를 결정할 수 있다

가. 빅데이터 변환의 이해
- 데이터 처리 및 분석을 효율적으로 잔행하기 위해서는 데이터 분석 목적에 따라 데이터를 변환 시킬 필요가 있다
- 데이터 변환은 정해진 규칙에 의해 바꾸는 것을 의미하며 특히 데이터를 수집하는 과정에서 컴퓨터가 바로 처리할 수 없는 비정형 데이터나 반정형 데이터를 전형 데이터의 구조적인 형태로 전환하여 저장하는 것을 의미
- 데이터의 변환 작업 이후 빅데이터를 효과적으로 분석하기 위해서 (Legacy) 데이터들을 우선 통합하고, 비정형 데이터를 정형 데이터로 변환하여 레거시 데이터와의 통합이 이루어짐

나. 데이터 전/후처리 단계
1) 개념
- 데이터 전처리 : 수집딘 데이터를 저장소에 적재하기 위해 데이터 필터링, 유형 변환, 정제 등의 기술을 사용하여 데이터를 변환하는 단계
- 데이터 후처리 : 저장된 데이터를 분석에 용이하도록 변환, 통합, 축소 등의 기술을 사용하여 가공하는 단계

2) 고려사항
	(가) 전처리 고려사항
	- 유형 분류 시, 분류 기준을 적용할 수 있는 기능 제공
	- 변환에 필요한 알고리즘 혹은 구조를 정의할 수 있는 기능 제공
	- 사용자 지정 기준에 맞게 변환되었는지 확인 기능 제공
	- 데이터 변환 실패 시 재시도 및 취소 가능 기능 제공
	- 실패 이력 저장 및 해당 내용을 사용자에게 전달하는 기능 제공
	- 결과 데이터 저장 기능 제공

	(나) 후처리 고려 사항
	- 이상값을 추세에 맞게 변환 또는 추천할 수 있는 기능 제공
	- 집계시, 데이터 요약 기능 제공
	- 특정 구간에 분포하는 값 추출 등에 대한 확인 기능을 통해 변환, 패턴, 이벤트를 감시할 수 있는 기능 제공
	- 원시 데이터셋과 변환 후 데이터 셋간의 변환 로그를 저장할 수 있는 기능 제공

다. 데이터 변환 기술
- 수집된 데이터를 분석에 용이하도록 정해진 규칙에 따라 일관성 있는 형식으로 바꾸는 것

| 기술                                        | 내용                                                                                                    |
| ----------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| 평활화<br>(Smoothing)                        | - 데이터로부터 잡음을 제거하기 위해 데이터 추세에서 벗어나는 값을 변환함<br>- 구간화, 군집화 방법을 이용                                        |
| 집계<br>(Aggregation)                       | - 다양한 차원의 방법으로 데이터를 요약<br>- 여러 개의 표본을 하나의 표본으로 줄이는 발법, 함수를 이용해 일괄적으로 변수 변환을 적용하여 새로운 변수로 값을 생성하는 방법 등 |
| 일반화<br>(Generalization)                   | - 특정 구간에 분포하는 값으로 스케일 변화<br>- 특정 데이터가 아닌 범용 데이터에 적합한 모델을 만드는 방법                                       |
| 정규화<br>(normalization)                    | - 데이터가 정해진 구간 내에 포함되도록 함<br>- 데이터에 대한 초소-최대 정규화, z-스코어 정규화, 소수 스케일링 등의 통계적 기법 적용                      |
| 속성 생성<br>(Attribute/Feature Construction) | - 데이터 통합을 위해 새로운 속성 및 특징을 만듦<br>- 주어진 여러 데이터 분포를 대표할 수 있는 새로운 속성 및 특징 활용                              |

라. ETL 프로세스
1) ETL *개념, DW를 활용한 데이터 관리 시스템에서의 ETL 역할 이해*
- 데이터의 이동 및 변환 절차와 관련된 업계 표준 용어
- ETL 프로세스는 서로 다른 시스템 간의 데이터 공유를 위한 가장 일반적인 형태의 하나. 기존 레거시 시스템 환경으로부터 빅데이터를 추출하여 비즈니스 데이터로 변환할 수 있도록 지원함
- 다양한 데이터 원천으로부터 추출 및 변환된 데이터를 운영데이터 스토어, 데이터웨어하우스, 데이터 마트 등에 적재하는 작업에서 핵심적 역할을 수행
- 데이터 이동 및 변환이 주된 목적이며 데이터 통합, 데이터 이동, 마스터 데이터 관리 등의 작업을 위해서도 활용됨
- 다양한 시스템들 간 대용량의 데이터 교환이 필요하거나 복잡한 비즈니스 룰이 적용되는 데이터 교환이 필요한 경우에 활용됨
- ETL 구현을 위한 다양한 상용 소프트웨어들이 나와 있으며 일괄 ETL나 실시간 ETL로 나뉜다

2) ETL의 역할
- 조직 내에서 데이터웨어하우스를 기반으로 데이터 관리가 이루어지는 경우 크게 데이터의 수집/관리/분석의 역할로 나눠지며 각 역할을 위해 3개의 레이어로 구성된다
- 소스 레이어는 데이터 수집과 관련된것으로 여기에서는 다양한 원천으로부터 데이터를 수집함
- 수집된 데이터는 DW레이어는 데이터웨어하우스로 전달되기 전에 우선 ETL과정을 거치게 됨. ETL 과정에서 수집된 데이터들 중 필요한 데이터를 추출하고 형식에 맞게 변환시킨 후, DB혹은 DW에 저장함
- 이후 DW 레이어에서는 데이터웨어하우스를 통해 조직 내 데이터를 체계적으로 관리하며 동시에 분석 레이어의 BI및 애널리틱스 등을 이용하여 의사결정에 필요한 적절한 데이터 분석도 이루어진다

| 소스 레이어 |         | DW 레이어                    |       | 분석 레이어                  |
| ------ | ------- | ------------------------- | ----- | ----------------------- |
| 정보계    | ETL 레이어 | 데이터 웨어 하우스                |       | BI                      |
| 기간계    | >       | ^^^^                      |       | 애널리틱스                   |
| 관리계    | >       | 빅데이터 플랫폼<br>ETL 데이터 통합 검색 | ----- | -----↑                  |
| 빅데이터   |         |                           |       | 레이어 : 계층<br>소스 : 데이터 원천 |

3) ETL 기능
- 논리적 데이터 변환
- 도메인 검증
- DBMS 간 변환
- 데이터 요약
- 데이터 키 값으로 시간 값의 추가
- 데이터 키 값의 재구성
- 레코드 통합
- 불필요한 데이터 삭제 및 중복 데이터 삭제

마. 데이터 변환 절차
- 비정형 데이터를 정형 데이터로 변환하여 저장할 때 관계형 DBMS을 가장 많이 사용
- 비정형 또는 반정형 데이터를 정형 데이터로 변환하는 과정
1) 데이터 구조 정의
- 다양한 데이터 원천으로부터 수집할 데이터들의 속성 구조 확인
- 데이터의 구조를 미리 정의하고 추후 체계적인 관리 및 활용을 위해 적절한 변수명을 부여하여 구분함
2) 수행 코드 정의
- 데이터 수집 절차와 관련된 수행 코드 정의
- 수집 및 추출 대상이 되는 정보의 위치 및 정보의 구조를 확인한 후 해당 데이터 추출
- 태그로 둘러싸인 웹페이지 정보의 경우에는 태그를 제외한 대상 데이터의 정보 구조를 확인하여 불필요한 부분을 제외하고 필요 데이터만을 추출할 수 있도록 함
3) 프로그램 작성
- 수집 및 추출한 데이터를 조직 내 RDBMS에 저장할 수 있는 프로그램 작성
4) DB저장
- 저장 프로그램을 실행시켜 수집 및 추출한 데이터를 DB에 저장
// 참고. 데이터 축소
데이터 변환의 한 측면으로 데이터 축소 방법을 이용할 수 있음. 데이터 축소는 데이터가 가진 고유 특성은 손상되지 안도록 하면서 분석에 불필요한 데이터를 축소하여 분석의 효율성을 향상시킬 수 있는 방법
(1) 숫자 축소 : 원 데이터량을 상대적으로 적은 표현 형태로 대체하는 방법. 회귀, 로그선형 모형, 히스토그램, 샘플링 등의 방법
(2) 차원 축소 : 고려중인 랜덤변수나 속성의 개수를 축소시키는 과정. 웨이블렛 변환 및 주성분분석(PCA) 이용 가능. PCA는 여러 속성 중 서로 높은 상관성을 가지는 선형 조합으로 새로운 변수를 만들어 요약 및 축소하는 방법
(3) 데이터 압축 : 원 데이너의 축소나 압축된 표현을 얻기 위해 사용/ 원 데이터 정보의 손실이 없는 Lose Less 기법과 원 데이터의 추정만 가능한 Lossy 기법이 있음. 숫자 축소와 차원 축소 기술 또한 데이터 압축 형태가 될 수 있음

### 데이터 비식별화
- 2020년 8월 '데이터 3법' 시행으로 인해 빅데이터 관련 산업의 관심이 집중되고 있는 분야. 프라이버시 침해를 최소화하면서도 빅데이터 분석 기술의 효용을 극대화할 수 있는 개인정보 활용을 위해 고안됨

가. 데이터 보안
- 데이터 유출을 방지하고 데이터의 안전한 활용을 위해서는 보안 관리를 위한 필수사항을 도출하고, 보안관리 조치가 이루어져야 함

1) 데이터 보안 고나리 주요 업무
- 보안관리 필수사항 도출 : 데이터 수집, 저장 처리, 데이터 분석 및 활용 단계에서 발생 가능한 보안 침해 가능성 및 개인정보처리 관련 사항 검토
- 보안관리 조치 : 관련 법 제도 및 지침서 등을 활용하여 데이터 보안 관리 조치

2) 데이터 보안 적용 기술

| 기술         | 내용                                                                                                                                                                              |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 사용자 인증     | - 누가 어떤 데이터에 어떤 조치를 취할 수 있는지를 사전에 정한 바에 따라 데이터 및 관리 시스템에 접근하려는 자의 접근 자격을 확인하는 것<br>- ex. ID/password, OTP, 전자인증, 통합사용자 인증 등                                                     |
| 접근제어       | - 누가 어떤 객체를 읽거나 입력, 실행하고자 할 때마다 그 주체가 해당 객체에 대한 권한이 있는지를 확인하고 통제하는 것<br>- ex. 강제 접근 제어, 임의 접근 제어, 역할 기반 접근 제어                                                                   |
| 암호화        | - 평문을 해독이 불가능한 형태로 변형(암호화)하거나 암호문을 해독 가능한 형태로 복원(복호화)하기 위한 원리, 수단, 방법 등을 취급하는 기술<br>- 암호화/복호화에 사용되는 키의 대칭 여부에 따라 대칭키 암호화와 비대칭키 암호화로 분류<br>- 암호화 알고리즘 : DES,AES. RSA. MDS, SHA 등 |
| 개인 정보 비식별화 | - 수집된 데이터에 포함된 개인 정보의 일부 또는 전부를 삭제하거나 혹은 다른 정보로 대체 또는 다른 정보와 결합하더라도 특정 개인을 식별하기 어렵도록 하는 일련의 조치                                                                                  |
| 개인 정보 암호화  | - 데이터베이스 전체에 대한 보호가 아닌 개인정보가 포함된 특정 필드에 대한 보호<br>- 암호화된 개인정보 저장 후에도 정상적으로 개인정보를 이요할 수 있도록 데이터 베이스를 안전하고 효율적으로 인덱싱하는 기술                                                          |

나. 비식별화 개념
- 데이터 내에 개인을 식별할 수 있는 정보가 있는 경우 이의 일부 또는 전부를 삭제하거나 일부를 속성 정보로 대체하여 처리함으로써 다른 정보와 결합하여도 특정 개인을 식별하기 어렵도록 하는 조치
- 국내에서 데이터 활용을 위한 움직임은 2016년 6월부터 구체화되었으며 국무조정실/행정안전부/방통위 등 다양한 관계 부처가 합동으로 '개인정보 비식별 조치 가이드라인'을 만들면서 시작

다. 비식별화 대상 및 기준
1) 비식별화 적용 대상
- 비식별화는 그 자체로서 개인을 식별할 수 있는 정보를 대상으로 함
- 해당 정보만으로는 특정 개인을 알아볼 수 없더라도 다른 정보와 쉽게 결합하여 개인을 알아볼 수 있는 정보 또한 포함됨

2) 비식별화 적용 시기
- 빅데이터 수집 및 활용의 이전 단계에서 개인정보가 식별되는 경우 혹은 이후 정보의 추가 가공 등을 통해 개인이 식별되는 경우에 적용

3) 비식별화 적용 시기의 예시
- 개인 정보의 수집 및 저장 시
- 개인 정보 포함 가능성이 있는 데이터의 활용 시
- 다른 기관 및 조직과의 정보 공유 시
- 기관 및 조직 내의 서로 다른 부서간의 정보 공유 시

라. 비식별화 기술
- 빅데이터 비식별화 기본 원칙에는 크게 식별방지(식별자 제거)와 추론 방지(프라이버시 모델 준수) 두 가지가 있다

1) 식별자 처리(일반적 기법)를 통한 식별 방지 기술 
*5가지 기법 처리방식 이해, 예시를 보고 어떤건지 알아야 함*
- 국내에서 공급되고 있는 비식별화 솔루션

| 처리<br>기법   | 내용                                                                          | 예시                                                      | 세부기술                                                      |
| ---------- | --------------------------------------------------------------------------- | ------------------------------------------------------- | --------------------------------------------------------- |
| 가명<br>처리   | 개인식별이 가능한 데이터에 <br>대하여 직접적으로 식별 할 수<br>없는 다른 값으로 대체                         | 홍길동, 35세, 서울 , <br>한국대 재학<br>> 임꺼정, 30대, <br>서울, 국제대 재학 | 1. 휴리스틱 가명화<br>2. 암호화<br>3. 교환 방법                         |
| 총계<br>처리   | 개인정보에 대하여 통계값<br>(전체 혹은 부분)을 적용하여<br>특정 개인을 판단할 수 없도록 함                     | 임꺽정 180, <br>홍길동 190<br>> 물리학과 학생 <br>키 합 370, 평균 165   | 4. 총계 처리<br>5. 부분 총계<br>6. 라운딩<br>7. 재배열                  |
| 데이터 <br>삭제 | 개인정보 식별이 가능한 <br>특정 데이터 값을 삭제                                               | 주민번호 901206-1234567<br>> 90년대 생, 남자                     | 8. 식별자 삭제<br>9. 식별자 부분삭제<br>10. 레크드 삭제<br>11. 식별 요소 전부 삭제 |
| 데이터<br>범주화 | 단일 식별 정보를 해당 그룹 <br>대표값으로 변환(범주화)하거나<br>구간값으로 변환(범위화)하여<br>고유 정보 추적 및 식별 방지 | 홍길동, 35세<br>> 홍씨, 30~40세                                | 12. 감추기<br>13. 랜덤 라운딩<br>14. 범위 방법<br>15. 제어 라운딩          |
| 데이터<br>마스킹 | 개인식별정보에 대하여 <br>전체 또는 부분적으로<br>대체값으로 변환                                     | 홍길동, 35세<br>> 홍**, 35세                                  | 16. 임의 잡음 추가<br>17. 공백과 대체                                |
// 참고. 비식별 처리 기법
*최근 출제 문제는 가명처리, 치환, 섭동 이라는 표현을 사용하기도 했음!*

17가지 세부 기술

| 세부기술             | 내용 및 주요 특징                                                                                                               |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------ |
| 휴리스틱 가명화         | 식별자에 해당하는 값들을 몇 가지 정해진 규칙으로 대체하거나 가공해 자세한 개인정보를 숨기는 방법                                                                   |
| 암호화              | 정보 가공 시 일정한 규칙의 알고리즘을 적용해 암호화함으로써 개인정보 대체                                                                                |
| 교환 방법            | 기존 DB 레코드를 사전 정해진 외부의 변수(항목) 값과 연계해 교환하는 방식. 주로 ID, 나이, 성별, 신체정보, 전화번호, 주소 등을 비식별 처리할 때 사용                               |
| 총계처리             | 데이터 전체 또는 부분을 집계하는 것. 나이, 신장, 카드사용액, 유동인구, 사용자 수, 제품 재고량 및 판매량 등에 활용                                                     |
| 부분총계             | 데이터 셋 내 일정부분 레코드만 총계처리하는 방법이며 다른 데이터 값에 비해 오차범위가 큰 항목을 통계값으로 변환                                                          |
| 라운딩              | 집계 처리된 값에 대해 라운딩 기준을 적용해 최종 집계 처리하는 방법으로 세세한 정보보다는 전체 통계정보가 필요한 경우 많이 사용                                                 |
| 재배열              | 기존 정보값을 유지하면서 개인이 식별되지 않도록 데이터를 재배열하는 방법. 개인의 정보를 타인의 정보와 뒤섞어서 개인이 식별되지않도록 함                                             |
| 식별자 삭제           | 원본 데이터에서 식별자를 단순 삭제하는 방법. 성명/전화번호/계좌번호 등에 활용                                                                             |
| 식별자 부분삭제         | 식별자 일부를 삭제하는 방식. 주소/위치정보/전화번호 등에 활용                                                                                      |
| 레코드 삭제           | 다른 정보와 뚜렷하게 구별되는 레코드 전체를 삭제하는 방법. 키/소득/질병 등 구별되는 값을 가진 정보 전체를 삭제                                                         |
| 식별요소 전부 삭제       | 식별자뿐만 아니라 잠재적으로 개인을 식별할 수 있는 속성자까지 전부 삭제하는 방법                                                                            |
| 감추기              | 명확한 값을 숨기기 위해 데이터의 평균 또는 범주값으로 변환                                                                                        |
| 랜덤라운딩            | 수치 데이터를 임의의수를 기준으로 올림 또는내림하는 방법. 나이/소득/지출액/유동인구 등에 활용                                                                    |
| 범위 방법            | 수치데이터를 임의의 수 기준의 범위로 설정하는 방법. 해당 값의 범위 또는 구간으로 표현                                                                        |
| 제어 라운딩           | 랜덤 라운딩 방법에서 어떠한 특정 값을 변경할 경우 행과 열의 합이 일치하지 않는 단점을 해결하기 위해 행과 열을 일치시키는 기법. 주로 나이/키/소득/지출액/위치정보 등에 활용                      |
| 임의 잡음 추가(섭동, 교란) | 개인정보에 임의의 숫자 등 잡음을 추가하는 방법. 지정된 평균과 분산의 범위 내에서 노이즈가 추가되므로 원본 데이터의 유용성을 해치지 않으나 노이즈 값은 데이터 값과는 무관하기 때문에 유효한 데이터로 활용할 수 없음 |
| 공백과 대체(치환)       | 특정항목 일부 또는 전부를 공백 또는 대체 문자로 바꾸는 방법. 주로 성명/생일/주민번호 등에 활용                                                                  |

2) 프라이버시 모델 기반 추론 방지 기술
- 추론방지 기술은 빅데이터 비식별화기본 원칙 중의 하나로, 여기서 프라이버시 모델이란 다양한 추론 공격에 대해 개인정보 추론 위험 정도를 확률적/정량적으로 제한하는 방법론을 의미

| 처리 기법 | 내용                                                                                       |
| ----- | ---------------------------------------------------------------------------------------- |
| k-익명성 | 특정인에 대한 추론 가능 여부를 검토하여 <br>일정 확률수준 이상 비식별 되도록 조치                                         |
| L-다양성 | k-익명성의 취약점을 보완한 것. <br>특정인에 대한 추론이 불가능한 것으로 판단되더라도 <br>민감한 정보의 다양성을 높임으로써 추론 가능성을 낮추는 기법 |
| t-근접성 | L-다양성 뿐만 아니라 민감한 정보의 분포를 낮추어 추론 가능성을 더욱 낮추는 기법                                           |

가) k-익명성
- 공개된 데이터에 대한 연결 공격 등의 취약점을 방어하기 위해 제안된 기법
- 일반적으로 활용하는 데이터에서는 이름, 주민번호 등과 같은 식별자를  삭제하고 공개하지만, 준식별자 값들의 조합을 통해 배포된 데이터의 개인이 추론되어 민감정보가 노출 될 수 있다. -> 연결 공격의 문제 발생 여지
- 이러한 연결 공격을 방어하기 위해 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재하도록 하여, 쉽게 다른 정보로 결합할 수 없도록 데이터를 구성하는 모델
- k-익명성을 통해 한 개인이 k명의 다른 사람(레코드)과 구별되지 않도록 민감하지 않은 속성을 수정할 수 있다. ex. '연령'을 범주화 함으로써 다른 데이터 소스로부터의 정보와 매칭을 시키더라도 특정 개인의 민감정보를 파악할 수 없도록 함

나) L-다양성
- k-익명성에 대한 동질성 공격 및 배경지식에 의한 공격을 방어하기 위해 제안된 기법
- 데이터가 k-익명성을 만족하더라도 각 블록에서 단 하나의 민감정보를 가지고 있다면 개인의 민감정보는 노출될 수 있다. L-다양성은 각 블록이 적어도 L개의 다양한 민감정보를 가지고 있어야 한다는 조건을 만족해야함. 여기서 블록은 데이터에서 민감하지 않은 속성 값이 동일한 레코드 집합
- 정보가 충분한 다양성을 가지게 되면, 다양성의 부족으로 인한 공격으로부터 방어가 가능하고, 배경 지식에 의한 공격에도 일정 수준의 방어능력을 갖추게 됨. 그로 인해 민감 속성을 쉽게 추론할 수 없게 됨

다) t-근접성
- L-다양성을 만족하더라도 모집단에 대비하여 민감정보의 분초 차이를 통해 개인 사생활 정보가 노출되는 문제 발생 여지. t-근접성은 쏠림 공격 및 유사성 공격에 대응하기 위해 제안된 기법
- t-근접성은 데이터 집합에서 구별되지 않는 레코드들의 민감한 정보의 분포와 전체 데이터의 민감한 정보의 분포의 차이를 t이하로 만들어 프라이버시를 보호하는 모델

라) 프라이버시 보호의 취약점 및 방어 기법
- 공개된 데이터의 취약점을 방어하기 위해 제안된 프라이버시 보호 모델


| 취약점        | 세부 취약점                                                                            | 방어 기법 |
| ---------- | --------------------------------------------------------------------------------- | ----- |
| 공개 데이터 취약점 | 개인정보를 포함한공개 데이터                                                                   | k-다양성 |
|            | 연결공격 : 식별자가 삭제 되었다고 해도 그외의 공개 정보들을 다른 데이터 소스로부터의 정보와 매치하여 특정 개인의 식별자를 추론할 수 있는 공격 |       |
| k-익명성 취약점  | 동질성 공격 : 데이터 집합에서 동일한 민감 정보를 이용하여 공격 대상의 민감한 정보를 알아내는 공격                          | L-다양성 |
|            | 배경지식공격 : 주어진 데이터 이외의 공격자의 배경지식을 통해 공격 대상의 민감한 정보를 알아내는 공격                         |       |
| L-다양성 취약점  | 쏠림 공격 : 민감한 정보가 특정한 값에 쏠려 있는 경우 취약점 발생                                            | t-근접성 |
|            | 유사성 공격 : 익명화된 레코드의 민감한 정보가 서로 비슷하다면 취약점 발생                                        |       |
https://12bme.tistory.com/163
마) 익명성 검증 조건 : m-유일성
- 익명처리 모델은 합리적으로 예상되는 모든 수단을 동원하더라도 한 개인을 식별하지 못하도록 원본 데이터셋을 익명 가공처리하는 것. 완벽하게 익명처리 된 데이터셋에서는 원본 데이터 셋의 모든 부분속성집합에 대해 유일성이 존재하지 않도록 해야 한다
- 이렇게 되면 익명 데이터셋만으로 원본의 특정한 개인을 식별하는 것이 불가능해지며 m-유일성을 이용해 익명처리 모델의 성질을 명확하게 정의할 수 있다
- 즉, 원본 데이터셋 테이블 s가 주어지고, 이를 완벽하게 익명처리한 익명데이터셋 테이블 t가 동일한 속성값을 갖는 레코드들이 존재한다면 원본 데이터셋 s에는 최소 m개 이상의 레코드들이 존재해야 함

바) 비식별화에서 사용되는 주요 개념
- 식별자 : 개인을 식별할 수 있는 속성들(1:1 대응이 가능한 모든 정보) 암호화된 값도 식별자로 분류됨. 비식별 조치 시 무조건 "삭제" 되어야 함
- 준식별자(QI) : 자체로는 식별자가 아니지만 다른 데이터와 결합을 통해 특정 개인을 간접적으로 추론하는 데 사용될 수 있는 속성들. 비식별화 기법들에서 변형/조작의 대상이 됨
- 민감정보(SA) : 개인의 사생활을 드러낼 수 있는 속성. 데이터 분석시 주로 측정되는 대상속성으로 대부분의 현대적 비식별화 기법들에서 데이터 값을들 보존함
*최근 기출 개념* 차등보호
데이터에 익명성을 더해 프라이버시 침해 위험을 감소시키는 일종의 수학적 정의. 차등 프라이버시는 인공지능 학습용 데이터에 포함된 개인정보를 보호하기 위해 해당 데이터 세트에 임의의 노이즈를 삽입함으로써 개인정보가 제3자에게 노출되지 않도록 보호하는 기법. 대규모 데이터 세트에서 개별 주체들의 개인정보 노출 위험을 최소화하면서 데이터를 활용할 수있도록 하는 것이 목적

### 데이터 품질 검증
가. 개념
1) 데이터 품질
- 조직의 목적 달성을 위해 관리되는 데이터가 조직 구성원, 고객 등 데이터 이용자의 만족을 충족시킬 수 있는 수준
- 데이터의 최신성, 정확성, 상호연계성 등을 확보하여 이를 사용자에게 유용한 가치를 줄 수 있는 수준
- 특정 비즈니스 목적에 특정 사실의 부합 여부를 결정하기 위해 사용되는 주관적인 기준
- 데이터가 관심 있는집단에서 사용되기 위해 요구되는 품질 특성을 충족하도록 보장해주는 일련의 지식체계 및 절차

2) 데이터 품질 관리
- 조직에서 보유한 DB에 저장되어 있는 데이터를 수집/처리/보관/분석하는 동안 무결성(Integrity)을 보장하는 비즈니스 프로세스(협의적 개념)
- 데이터 관리 비전/목표/전략/데이터 관리 원칙과 기준, 데이터 관리 절차 등을 모두 포괄하는 데이터관리(거버넌스)체계(광의적 개념)
*최근 기출개념* 고품질 데이터의 5가지 특징
1. 관련성 : 데이터 이용자의 요구사항을 충족하는 정도
2. 정확성 : 측정하고자 하는 모집단의 특성이나 크기를 얼마나 정확하게 측정했는가를 평가
3. 시의성/정시성 : 통계의 현실 반영도와 예고된 공표 시기를 정확히 준수하는지 평가
4. 비교성/일관성 : 시/공간이 달라도 통계가 동일한 기준으로 집계되어 서로 비교가 가능한지, 동일한 현상에 대해 서로 다른 기초 자료나 작성주기 등에 의해 작성된 통계자료들이 얼마나 유사한 결과를 보이는지 평가
5. 접근성/명확성 : 이용자가 통계자료에 얼마나 쉽게 접근할 수 있는지, 통계가 어떻게 만들어졌는지에 대한 정보제공 수준

나. 데이터 품질 관리의 중요성

| 구분               | 내용                                |
| ---------------- | --------------------------------- |
| 데이터 분석결과의 신뢰성 확보 | 데이터 품질에 따라 분석 과정 및 결과의 품질을 좌우     |
| 일원화된 프로세스        | 데이터 분석을 위한 업무 처리 및 데이터 관리의 효율성 향상 |
| 데이터 활용도 향상       | 고품질 데이터 확보로 데아토 활용도 향상            |
| 양질의 데이터 확보       | 불필요한 데이터를 제거 > 고품질 데이터 확보 비율 향상   |

다. 데이터의 품질 기준 *품질 평가 기준에 대해 숙지*
1) 정형 데이터의 품질 기준

| 품질 수준 | 내용                                        |
| ----- | ----------------------------------------- |
| 완전성   | 필수항목에 누락이 없어야 함                           |
| 유일성   | 데이터 항목은 유일해야 하며 중복되어서는 안됨                 |
| 유효성   | 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족해야 함        |
| 일관성   | 데이터가 지켜야 할 구조, 값, 표현되는 형태 일관되게 정의되고 서로 일치 |
| 정확성   | 실세계에 존재하는 객체의 표현 값이 정확히 반영 되어야 함          |
- 국내외 데이터 품질 관련 문헌 및 데이터 품질관리 컨설팅 기관의 자료들을 종합적으로 정리해보면, 정형 데이터에 대한 품질기준은 완전성 유일성 유효성 일관성 정화성 5가지로 요약됨
- 하지만 일반적인 데이터 품질 기준은 조직에서 필요한 품질관리 실무 기준으로 활용하기에는 부족할 수 있음. 이를 해결하기 위해 실제 기업에서는 앞서 요약한 5가지의 데이터 품질 기준을 보다 상세화하고 그 의미를 세분화하여 하위 품질 기준을 정의할 수 있다

2) 비정형 데이터의 품질기준

| 품질기준 | 내용                                           |
| ---- | -------------------------------------------- |
| 신뢰성  | 규정된 조건 내에서 신뢰수준을 유지하고 사용자의 오류를 방지            |
| 기능성  | 특정 조건에서 사용 시 명시된 요구 및 내재된 요구사항 충족 기능을 제공해야 함 |
| 효율정  | 규정된 조건내에서 사용되는 자원의 양에 대비해 요구된 적정 성능 제공       |
| 사용성  | 데이터를 사용 및 분석하게 될 이용자에 의해 충분히 이해, 선호될 수 있어야 함 |
| 이식성  | 다양한 환경 및 상황에서 해당 데이터들이 실행할 수 있어야 함           |

라. 품질 진단 방법
- 정의된 품질 지표들을 어떤 방법으로 측정할 것인가에 대한 기준이 되는 진단 방법

| 품질진단방법    |          | 설명                                                                                                                               |
| --------- | -------- | -------------------------------------------------------------------------------------------------------------------------------- |
| 프로파일링     | 값 진단     | 데이터 값의 유효성, 정확성 등 데이터 값 자체 오류를 분석하는 방법<br>- Column 분석, 날짜분석, 패턴분석, 코드 분석 등을 통해 데이터 값의 정확성을 중심으로 진단                               |
|           | 구조<br>진단 | 논리적 데이터 구조의 오류로 인한 일관성, 정합성 등을 확보하지 못하는 결함을 분석하고 진단하는 방법<br>- 표준화 수준(코드, 도메인 등), 테이블 구조, 정규화 수준, Column 및 관계 정의 등 데이터의 구조적 결하 측정 |
| 체크리스트     |          | 전반적인 데이터 품질관리 수준과 지표별 데이터 품질 수준을 체크리스트(설문 또는 인터뷰)를 통해 진단하는 방법                                                                    |
| 업무규칙 진단   |          | 법, 규정에 정의된 업무기준(산출식)에 근거하여 데이터가 관리되고 있는 지에 관한 측정 스크립트(SQL 등)를 실행하여 오류 값을 추출함                                                     |
| 비정형<br>실측 |          | 문서, 이미지, 동영상 등 정형화 되어 있지 않은 정보를 사람이 직접 확인(실측)을 통하여 오류 여부를 진단하는 방법<br>- 별도 도구 없이 직접 정보를 조회하거나 해당 문서를 수기로 확인                       |

## 데이터 적재 및 저장
### 데이터 적재
Loading은 데이터 수집과 변환과 관련되는 ETL 프로세스의 마지막 과정

가. ETL 프로세스 설계 중 적재 방안
- 운영 시스템을 비롯한 다양한 데이터 원천으로부터 데이터를 추출하여 변환 및 적재과정을 설계할 때는 어떤 요구조건을 충족해야 하는지를 파악해야함
- 이 과정에서는 소스 시스템을 분석하고, 데이터 변환 규칙을 확정하며, 타겟 시스템 데이터의 적재 방법은 선정해야 함
- 특히, 데이터 적재 과정에서는 데이터의 신뢰성 확보를 위한 데이터 오류 대책 방안 및 데이터 검증 방안을 마련할 필요가 있다. 데이터 검증 방안을 위해 데이터 이행 후 별도의 로그 처리 및 비교 검증 프로세스를 추가하는 등 데이터 적재 담당자의 노력이 필요함

나. 데이터 마이그레이션 ETL설계
- ETL의 마지막 단계 적재 과정을 성공적으로 수행하기 위해서는 추출과 변환 과정도 중요
- '소스 시스템 분석과정'에서는 이행데이터의 식별 과정을 거쳐 삭제 데이터 및 변경 데이터 관리와 데이터 변경 구간의 범위 및 변경 일자 등과 같은 소스데이터 변경에 대한 타임스탬프 관리가 이루어져야 함. 이렇게 타임스탬프가 관리가 된다면 해당 Column을 이용해 변경데이터만을 적재할 수 있다
- '데이터 변환 규칙 설정 과정'에서는 데이터의 표준화 및 정합성을 위해 오류데이터 검출, Null 데이터 처리 방안, 코드 통합 등의작업 규칙을 확립해야 함. 이를 통해 표준화된 데이터들이 선별되어 데이터베이스 및 DW에 적재될 수 있다
- '타겟 시스템에 적대 방안'은 데이터 소스 시스템의 변환데이터 식별 가능 여부에 따라 Delete/Inset, Update/Insert, Truncate/Insert 등의 여러 적재 방법들 중에서 상황에 맞는 방법을 선택하여 적용하도록 한다
- '신뢰성 확보 방안'은 ETL과정에 대한 오류 발생을 비롯한 다양한 문제점을 파학하는것. 이 단계에서는 적재 건수, 초조 무결성 검증, 코드 검증등 데이터 검증 방안을 싱행함으로써 필요 데이터만을 타겟 시스템에 적재할 수 있도록 한다

### 데이터 저장
가. 빅데이터 저장 기술
- 대용량의 데이터 저장을 위한 관련 기술은 분산 파일 시스템, NoSQL, 병렬 DBMS, 클라우드 파일 저장 시스템, 네트워크 구성 저장 시스템 등이 있다
- 빅데이터 저장 기술은 구글, 애플 등이 상당히 완성도 있게 개발해서 활용하고 있으며 세계적으로는 하둡의 HDFS와 HBase, 아파트 카산드라 등이 대표적 솔루션으로 활용되고 있다. 국내는 ETRI(한국전자통신연구원)의 GLORY-FS 등이 있다

1) 분산 파일 시스템(Distributed File System)
- 네트워크를 통해 물리적으로 다른 위치에 있는 여러 컴퓨터에 자료를 분산 저장하여 마치 로컬 시슽ㅁ에서 사용하는 것처럼 동작하게 하는 시스템
- 대용량의 데이터를수집, 저장, 분석하는 대 두 대이상의 컴퓨터를 두어 처리 시간과 비용을 줄일 수 있으며, 일부 작업에 문제가 생기는 경우 해당 부분만을 재처리할 수 있다

가) 하둡(Hadoop)
- 분산 컴퓨팅 환경을 지원하는 가장 대표적인 도구
- 2005년 더그 커핑과 마이크 카파렐라가 공동이로 개발한 것. 구글과 유사한 부분이 많다
- 하둡 소프트웨어 라이브러리는 단일 서버에서 수천대의 머신으로 확장할 수 있게 설계된 자바 오픈소스 프레임워크로, 간단한 프로그래밍 모델을 이용해서 분산된다수의 컴퓨터 클러스터에서 대규모 데이터 세트를 처리할 수 있게 한다
- 하둡은 신뢰할 수 있고 확장이 용이하며 분산 컴퓨팅 환경을 지원하는 오픈소스 소프트웨어

나) 하둡 분산 파일 시스템(HDFS)
- 클라우드 컴퓨팅 환경을 구축하기 위해 이용하며 대용량 데이터의 분산 저장 기능을 제공하는 시스템
- 다수의 리눅스 서버에 설치되어 운영되며 뛰어난 확장성으로 페타바이트 이상의 대용량 데이터 저장 공간을 확보할 수 있다
- 내고장성(fault-tolerant)을 위해 고비용의 하드웨어를  사용하는 기존의 서버 구성방식과는 다르게 리눅스 장비를 사용함으로써 RDBMS에 비해 시스템 구축비용이 저렴함
- 자바로 구현되어 다양한 서버에서 구동 가능
- 파일 생성, 삭제, 수정 등은 가능하지만 사용자의직접 접근 권한은 지원하지 않음
- GFS를 모델로 해서 만들어진 오픈소스기 때문에 GFS와 동일한 특징을 가지며 소스코드도 유사

//참고 HDFS 클러스터 구성 방식
마스터-워커(master-worker) 패턴으로 동작하는 두 종류의 노드(마스터인 하나의 네임노드와 워커인 여러 개의 데이터 노드)로 구성되어 있다) HDFS 클라이언트가 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근한다. HDFS클라이언트는 POXLS(Portable Operation System Interface)와 유사한 파일 시스템 인터페이스를 제공하기 때문에 사용자는 네임노드와 데이터노드에 관련된 함수를 몰라도 코드를 작성할 수 있다

다) 구글파일시스템(GFS)
- 구글이 자사 사용 목적으로 개발한 분산 파일 시스템. 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 시스템
- 일반적 파일 시스템에서의 클러스터 및 섹터와 유사하게 64MB로 고정된 크기의 청크 로 파일을 나눈다. 청크 서비들은 데이터를 자동으로 복사하여 저장하고, 주기적으로 청크 서버의 상태를 마스터에게 전달한다
- 시스템과 시스템간의 상호작용을 위한 설계로서 저렴한 범용 컴퓨터들로 구성되며 이러한 환경에서도 하드웨어 안정성을 보장받을 수 있으며, 자료 유실을 고려하여 시스템이 설계됨

//참고. 구글 빅테이블
- NlSQL 데이터베이스로 대용량의 데이터를 온라인에 저장할 수 있는 서비스. 오픈소스 아파치 HBase용 API를 사용해서 데이터를 읽고 작성할 수 있기 때문에 빅데이터용 인기 오픈소스 데이터 프로세스 플랫폼인 하둡 소프트웨어로 쉽게 이 서비스를 이용할 수 있다. 또한 구글 빅쿼리, 구글 클라우드 데이터플로우 등 다양한 구글 클라우드 서비스와도 통합할 수 있다. 해당서비스를 이용해서 금융업체들은 페타바이트 용량의 거래 데이터를 저장하여 트랜드 분석에 활용할 수 있으며 그 외 데이터 집적도가 높은 없계에서도 활용할 수 있다

2) NoSQL
- 기존의 오라클 등을 중심으로 개발/발전되어 온 RDBMS 중심의 데이터 저장 기술로는 비정형 데이터의 저장과 관리가 힘들다는 한계로 등장한 새로운 데이터 저장 기술
- Non-SQL or Not-only-SQL의 줄임말. 키 값을 이용하여 데이터를 간단하게 저장하고 데이터 저장 및 관리 시 SQL을 사용하지 않음
- 대표적인 방법으로 Cassandram HBase 등이 있다

가) Cassandra
- 대용량의 데이터 저장 및 처리가 가능한 NoSQL 공개 소프트웨어로 SQL을 사용하지 않는 대용량의 고성능 처리 시스템
- 횡적인 용량 확정으로 노드를 추가할 수 있으며 분산 시스템 환경에서 대량의 데이터를 처리할 수 있도록 지원함

나) HBase
- 구글 빅테이블을 본보기로 자바 기반 개발된 비관계형 데이터베이스
- 스키마 지정 및 변경 없이 데이터를 저장할 수 있으며 하둡 분산파일 시스템에서 동작함으로써 확장성이 보장되는 시스템
- 대용량의 데이터를 안정적으로 다루는데 효과적이며 전체 데이터에 대한 일관성을 보장
- 네이버 라인 메신저에 적용하기도 했다

| 기술             | 특징                                                                                                       | 제품                        |
| -------------- | -------------------------------------------------------------------------------------------------------- | ------------------------- |
| 분산파일시스템        | 네트워크를 통해 공유하는 여러 호스트 컴퓨터의 파일에 접근할 수 있돍 지원하는 파일 시스템                                                       | HDFS, GFS                 |
| NoSQL          | 기존 RDBMS 주요 특성인 ACID(Automicity, Consistency, Isolation, Durability)는 제공하지 않지만 뛰어난 확장성 및 성능을 제공하는 저장 시스템 | HBase, Cassandra          |
| 병렬 RDBMS       | 전통적인 RDBMS에서 발전한 형태. 수평 확장 접근 방식을 사용하는 기술                                                                | VoltDB, SAP HANA, Vertica |
| 클라우드 파일 저장 시스템 | 클라우드 환경에서 활용 가능한 가상화 기술을 이용한 분산 파일 시스템                                                                   | AWS S3                    |
| 네트워크 구성 저장 시스템 | 여러 종류의 데이터베이스를 하나의 데이터 서버에서 총괄적으로 관리하는 시스템                                                               | SAN, NAS                  |
*******
// - [**원자성**(Atomicity)](https://ko.wikipedia.org/wiki/%EC%9B%90%EC%9E%90%EC%84%B1_(%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4_%EC%8B%9C%EC%8A%A4%ED%85%9C) "원자성 (데이터베이스 시스템)")은 트랜잭션과 관련된 작업들이 부분적으로 실행되다가 중단되지 않는 것을 보장하는 능력이다. 예를 들어, 자금 이체는 성공할 수도 실패할 수도 있지만 보내는 쪽에서 돈을 빼 오는 작업만 성공하고 받는 쪽에 돈을 넣는 작업을 실패해서는 안된다. 원자성은 이와 같이 중간 단계까지 실행되고 실패하는 일이 없도록 하는 것이다.
- **정합성**(Consistency)은 트랜잭션 처리 전과 처리 후 데이터 모순이 없는 상태를 유지하는 것을 의미한다. 무결성 제약이 모든 계좌는 잔고가 있어야 한다면 이를 위반하는 트랜잭션은 중단된다.
- 독립성(Isolation)은 트랜잭션을 수행 시 다른 트랜잭션의 연산 작업이 끼어들지 못하도록 보장하는 것을 의미한다. 이것은 트랜잭션 밖에 있는 어떤 연산도 중간 단계의 데이터를 볼 수 없음을 의미한다. 은행 관리자는 이체 작업을 하는 도중에 쿼리를 실행하더라도 특정 계좌간 이체하는 양 쪽을 볼 수 없다. 공식적으로 고립성은 트랜잭션 실행내역은 연속적이어야 함을 의미한다. 성능관련 이유로 인해 이 특성은 가장 유연성 있는 제약 조건이다. 자세한 내용은 관련 문서를 참조해야 한다.
- **지속성**(Durability)은 성공적으로 수행된 트랜잭션은 영원히 반영되어야 함을 의미한다. 시스템 문제, DB 일관성 체크 등을 하더라도 유지되어야 함을 의미한다. 전형적으로 모든 트랜잭션은 로그로 남고 시스템 장애 발생 전 상태로 되돌릴 수 있다. 트랜잭션은 로그에 모든 것이 저장된 후에만 commit 상태로 간주될 수 있다.

나. 빅데이터 저장을 위한 고려사항
- 빅데이터 저장 및 관리에 고비용이 소요되는 문제를 해결하기 위해 저장 단가를 절감할 수 있는 비용 문제가 대두됨. 장비 구입비 뿐만아니라 비정형의 반구조적, 비구조적 데이터들도 수집 및 관리되어야 하므로 RDBMS 등의 방법으로는 관리가 어렵다. 따라서 다양한 형태의 데이터를 관리할 수 있는 기술이 필요하다
- 빅데이터에는 음성, 오디오, 영상 자료들도 포함되어 조직 내에 확보되는 데이터의 양은 기하급수적으로 늘어날 수 있다. 따라서 저장 용량을 수평적으로 쉽게 확장할 수 있어야 한다

| 문제점                                                                                   | 문제 해결을 위한 방안                                                                                        |
| ------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| - 데이터 저장 및 관리에 소요되는 높은 비용<br>- 다양한 형식 및 대용량 데이터의 효과적인 관리 문제<br>- 저장 용량 한계에 따른 확장의 어려움 | - 데이저 저장 관련 총소유 비용(TCO)을 낮출 수 있는 기술력 보유<br>- 데이터의 형식 및 용량에 크게 구애받지 않는 기술적 여유<br>- 횡적인 용량 확장의 용이성 향상 |

// 알아두기. 맵리듀스(MapReduce) 기능
Hadoop 클러스터의 데이터를 처리하기 위한 시스템으로 여러 노드에 태스크를 분배하는 방법. 크게 맵과 리듀스 두 단계로 구성되며, 맵과 리듀스 사이에는 shuffle과 sort 스테이지가 존재한다. 각 map task 는 전체 데이터셋에 대해 일부분에 대한 작업을 수행하며 보통은 하나의 HDFS block을 대상으로 수행한다
맵핑단계에서 여러 부분으로 나뉘어져 처리된 task는 shuffle과 sort 작업을 거치면서 재조합되어 리듀스 단계에서 간추려지고 이후 최종결과를 도출한다
![[Pasted image 20240315203602.png]]

다. DW와 데이터레이크
1) DW
	가) 정의
	- 사용자의 의사결정 지원을 위해 데이터를 분석가능한 형태로 저장한 중앙저장소. 정보와 창고의 합성어이다.
	- DW는 기존 정보를 활용해 더 나은 정보를 제공하고, 데이터의 품질을 향상 시키며, 조직의 변화를 지원하고 비용과 자원관리의 효율성을 향상시키는 것이 목적
	- ETL과정이 다양한 데이터 원천으로 부터 추출, 변환하여 DW에 적재 하는 것
	나) 특징
	- 통합성 : 기존 운영 시스템이 부서 혹은 기괄별로 따로 데이터를 중복 관리 하였던 것이 비해, DW는 다양한 데이터 원천으로부터의 데이터를 모두 통합하여 관리함 데이터 속성의 이름, 단위 등의 일관성을 유지
	- 주제지향성 : 기능 및 업무 중심(ex. 대출, 재고관리 등)이 아닌 주제 중심(ex. 고객, 공급자, 상품 등)으로 구성. 최종 사용자가 이해하기 쉬운 형태를 가짐
	- 시계열성 : 기존 운영 시스템은 최신 데이터를 유지하는데 반해, DW는 시간에 따른 변경 이력 데이터를 보유함
	- 비휘발성 : DW에 저장되는 데이터는 삭제 및 변경되지 않고, 일단 적재가 완료되면 읽기 전용 형태의 스냅 샷 데이터로 존재함

2) 데이터레이크(Data Lake)
	가) 개념
	- 빅데이터를 효율적으로 분석하고 사용하기 위해 다양한 영역의 원데이터를 한 곳에 모아서 관리하고자하는 목적에서 등장
	- 수많은정보 속에서 의미있는 내용을 찾기위해 방식에 상관없이 데이터를 저장하는 시스템으로, 대용량의 정형 및 비정형 데이터를 저장할 뿐만 아니라 접근도 쉽게 할 수 있는 대규모 저장소를 의미
	- Apache Hadoop, Teradata Integrated Big Data Platform 1700 등과 같은 플랫폼으로 구성된 솔루션을 제공하고 있다
	나) 특징
	- ETL 과정이 필요 없다. DW는 다양한 DB로부터 데이터를 추출한 후 정의된 스키마에 맞추어 변환 과정을 통해 적재하는 과정이 이루어짐. 하지만 DL은 형식에 상관없이 데이터를 저장하기 때문에 데이터 스키마를 맞추는 중간 작업이 필요 없다.
	- 유연성이 있다 .하나의 데이터 모델링에 국하되지 않고 여러 데이터 모델링에 대응할 수 있으며, 원 데이터 형태를 그대로 유지함으로써 빠르게 데이터 스트림을 활용한 데이터 분석을 실행할 수 있다
	- 확정성이 있다. 클라우드를 이용하여 확장이 용이하므로 갑작스럽게 데이터 양이 급증하더라도 그에 대해 빠르게 유연하게 대응할 수 있다
	- 하지만, 분석이 용이한 상태로 데이터를 저장하는 DW와는 달리, DL은 원 데이터를 그대로 저장하기 때문에 데이터 분석 시 DL에 직접 접근하여 분석하는 것은 어렵다. 이러한 단점을 해결하기 위해서는 분산 저장 프레임워크 및 메타데이터 관리 솔루션등을 이용할 수 있다

// 알아두기. 데이터 댐
문재인 정부 과기부에 추진되었던 데이터 구축사업, 그 사업에 따라 구축된 데이터들을 의미. 정부에서 자연스럽게 발생되는 데이터뿐만 아니라 실제로 민간에서 필요로 하는 데이터, 즉 자금력이 충분한 대기업들이 자신들의 사업 역량 강화를 위해서 돈 주고 만들어 내가나 외부에서 비싸가 사오는 그런 데이터들을 정부가 만들어 공급한다는 계획으로 민간에게 데이터를 공급하기 위한 프로젝트

라. 데이터 거버넌스
1) 개요
	가) 전사 차원의 모든 데이터에 대하여 정책 및 지침, 표준화, 운영조직 및 책임 등의 표준화된 관리체계를 수립하고 운영을 위한 프레임워크 및 저장소를 구축하는 것
	나) 마스터 데이터, 메타데이터, 데이터 사전은 데이터 거버넌스의 중요한 관리대상이다
	- 기업은 데이터 거버넌스 체계를 구축함으로써 데이터의 가용성, 유용성, 통합성, 보안성, 안정성을 확보
	- 데이터 거버넌스는 이러한 데이터 거버넌스의 체계에 대하여 빅데이터의 효율적인 관리, 다양한 데이터의 관맃계, 데이터 최적화, 정보보호, 데이터 생명주기 관리, 데이터 카테고리별 관리 책임자 지정 등을 포함함

2) 구성요소
	가) 개요
	- 원칙 조직, 프로세스는 유기적으로 조합하고 효과적으로 관리하여 데이터를 비즈니스 목적에 부합하도록 하고 최적의 정보 서시브를 제공할 수 있도록 한다
	나) 구성 3요소
		(1) 원칙
		- 데이터를 유지/관리하기 위한지침과 가이드
		- 보안, 품질 기준, 변경 관리
		(2) 조직
		- 데이터를 관리할 조직의 역할과 책임
		- 데이터 관리자, 데이터베이스 관리자, 데이터 아키텍트
		(3) 프로세스
		- 데이터 관리를 위한 활동과 체계
		- 작업 절차, 모니터링 활동, 측정 활동

3) 데이터 거버넌스 체계	
	가) 데이터 표준화
	- 데이터 표준 용어 설정, 명명 규칙 수립, 메타 데이터 구축, 데이터 사전 구축 등
	- 데이터 표준 용어는 표준 단어 사전, 표준 도메인 사전, 표준 코드 등으로 구성되며 사전간 상호 검증이 가능하도록 점검 프로세스를 포함해야 함
	- 명명 규칙은 필요시 언어별로 작성되어 매핑 상태를 유지해야 함
	나) 데이터 관리 체계
	- 데이터 정합성 및 활용의 효율성을 위하여 표준 데이터를 포함한 메타데이터와 데이터사전의 관리 원칙을 수립
	- 수립된 원칙에 근거하여 항목별 상세한 프로세스를 만들과 관리와 운영을 위한 담당자 및 조직별 역할과 책임을 상세히 준비
	- 빅데이터의 경우 데이터양의 급증으로 데이터의 생명주기 관리방안을 수립하지 않으면 데이터 가용성 및 관리비용 증대 문제에 직면
	다) 데이터 저장소 관리
	- 메타데이터 및 표준 데이터를 관리하기 위한 전사차원의 저장소를 구성
	- 저장소는 데이터 관리 체계 지원을 위한 워크플로우 및 관리용 응용 소프트웨어를 지원하고 관리대상 시스템과의 인터페이스를 통한 통제가 이루어져야 함
	- 데이터 구조 변경에 따른 사전 영향 평가도 수행되어야 효율적인 활용이 가능
	라) 표준화 활동
	- 데이터 거버넌스 체계를 구축한 후 표준 준수 여부를 주기적으로 점검하고 모니터링 실시
	- 거버넌스 조직 내 안정적 정착을 위한 계속적인 변화 관리 및 주기적 교육 진행
	- 지속적인 데이터 표준화 개선 활동을 통해 실용성을 높여야 함
- *최근 기출개념* 데이터 리터러시와 IT거버넌스
- - 데이터 리터러시 : 데이터를 읽고 이해하고 분석하며 비판적으로 수용 및 활용할 수 있는 능력. 하위 역량에는 데이터 수집, 데이터 관리, 가공 및 분석, 시각화, 데이터 기획 등이 있다
- IT 거버넌스 : 정보기술을 관리하는것. 전사 사럽 전략 및 경영 목표를 달성할 수 있도록 IT자원의 효율적 활용을 위한 관리/통제 체계 및 활동